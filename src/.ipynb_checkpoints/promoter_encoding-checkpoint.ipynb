{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6169c8c4-6c46-41c1-9c84-3bff9d2e2c8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sbol2\n",
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "import zipfile\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "current_dir = os.path.abspath('')\n",
    "data_path = os.path.join(current_dir, '..', 'data')\n",
    "attachments_path = os.path.join(current_dir, '..', 'attachments')\n",
    "pulled_attachments_path = os.path.join(current_dir, '..', 'pulled_attachments')\n",
    "sbol_path = os.path.join(current_dir, '..', 'sbol_data')\n",
    "downloaded_sbol_path = os.path.join(current_dir, '..', 'downloaded_sbol')\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b881c7-b711-41ab-af99-5c3e0dcb7245",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(data_path, \"frag-rLP5_LB_expression.txt\"), delimiter=\" \")\n",
    "# Expression levels from genomic fragment MPRA (random 200–300 bp sheared fragments) in LB media\t\n",
    "df2 = pd.read_csv(os.path.join(data_path, \"frag-rLP5-M9_expression.txt\"), delimiter=\" \")\n",
    "# Expression levels from genomic fragment MPRA (random 200–300 bp sheared fragments) in M9 media at rLP5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2d2488-94ef-4002-a271-e0e8813de09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cffdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761f55c1-a68b-485f-826f-09a11af50b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sbol_doc(df, file_path, media=\"LB\"):\n",
    "    doc = sbol2.Document()\n",
    "    sbol2.setHomespace('http://github.com/cywlol/promoters')\n",
    "    doc.displayId = \"E_coli_promoters\"\n",
    "    # save labels for later when we do the attachments through api\n",
    "    exp_data_labels = []\n",
    "    attachment_file_names = []\n",
    "\n",
    "    media_label_MD = media\n",
    "    chassis_label_MD = \"E_coli_chassis\"\n",
    "\n",
    "    # define the media once, then have it as a reference for each promoter row\n",
    "    media_md = sbol2.ModuleDefinition(media_label_MD)\n",
    "    doc.addModuleDefinition(media_md)\n",
    "    media_md.addRole(\"http://identifiers.org/ncit/NCIT:C48164\") \n",
    "    \n",
    "    # define the chassis once, then have it as a reference for each promoter row\n",
    "    chassis_md = sbol2.ModuleDefinition(chassis_label_MD) \n",
    "    doc.addModuleDefinition(chassis_md)\n",
    "    chassis_md.addRole(\"http://identifiers.org/ncit/NCIT:C14419\")\n",
    "    \n",
    "    #attach genome \n",
    "    chassis_md.wasDerivedFrom = [\"https://www.ncbi.nlm.nih.gov/Taxonomy/Browser/wwwtax.cgi?id=511145\"]\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        fragment_seq = row[\"fragment\"]\n",
    "\n",
    "        # Establish the identities of each component, ensuring that each one is unique\n",
    "        promoter_label_CD = f\"promoter_{i}\"\n",
    "        promoter_seq_label = f\"promoter_seq_{i}\"\n",
    "        sample_design_label_CD = f\"sample_design_{i}\"\n",
    "        strain_label_MD = f\"strain_{i}\"\n",
    "        data_label = f\"exp_data_{i}\"\n",
    "        exp_label = f'sample_{i}_expression_data'\n",
    "        attachment_file_name = f'frag_{media}_exp_sample_{i}.csv'\n",
    "\n",
    "        #engr_region_label_CD = f\"engr_region_{i}\"\n",
    "        #location_promoter_annotation = f\"location_promoter_annotation_{i}\"\n",
    "        #location_promoter_label = f\"location_promoter_label_{i}\"\n",
    "        \n",
    "        # Promoter and Sequence \n",
    "        promoter_cd = sbol2.ComponentDefinition(promoter_label_CD, sbol2.BIOPAX_DNA)\n",
    "        promoter_cd.roles = [sbol2.SO_PROMOTER]\n",
    "        seq = sbol2.Sequence(promoter_seq_label, fragment_seq, sbol2.SBOL_ENCODING_IUPAC)\n",
    "        doc.addSequence(seq)\n",
    "        promoter_cd.sequences = [seq.persistentIdentity]\n",
    "        doc.addComponentDefinition(promoter_cd)\n",
    "\n",
    "        \n",
    "        # Add strain module definition\n",
    "        strain_md = sbol2.ModuleDefinition(strain_label_MD)\n",
    "        strain_md.addRole(\"http://identifiers.org/ncit/NCIT:C14419\")\n",
    "        doc.addModuleDefinition(strain_md)\n",
    "        strain_c1 = strain_md.modules.create('chassis')\n",
    "        strain_c1.definition = chassis_md.persistentIdentity\n",
    "        strain_c2 = strain_md.functionalComponents.create('promoter')\n",
    "        strain_c2.definition = promoter_cd.persistentIdentity\n",
    "    \n",
    "        #annotation = sbol2.SequenceAnnotation(\"promoter_location\")\n",
    "        #range = sbol2.Range(\"prange\", start, end)\n",
    "    \n",
    "        #if (strand == \"-\"):\n",
    "        #    range.orientation = sbol2.SBOL_ORIENTATION_REVERSE_COMPLEMENT\n",
    "                \n",
    "        #annotation.locations.add(range)\n",
    "        #promoter_cd.sequenceAnnotations.add(annotation)\n",
    "        '''\n",
    "        # Engineered Region \n",
    "        engineered_cd = sbol2.ComponentDefinition(engr_region_label_CD, sbol2.BIOPAX_DNA)\n",
    "        engineered_cd.roles = [\"https://identifiers.org/so/SO:0000804\"]\n",
    "        sub = engineered_cd.components.create('promoter')\n",
    "        sub.definition = promoter_cd.persistentIdentity\n",
    "        doc.addComponentDefinition(engineered_cd)\n",
    "        # No sequence?\n",
    "        '''\n",
    "        \n",
    "        #strain_c2 = strain_md.functionalComponents.create('engineered_region')\n",
    "        #strain_c2.definition = strain_md.persistentIdentity\n",
    "        \n",
    "        # Sample Design  \n",
    "        sample_md = sbol2.ModuleDefinition(sample_design_label_CD)\n",
    "        doc.addModuleDefinition(sample_md)\n",
    "        sample_md.addRole(\"http://identifiers.org/obo/OBI:0000073\")\n",
    "\n",
    "        m_strain = sample_md.modules.create('strain')\n",
    "        m_strain.definition = strain_md.persistentIdentity\n",
    "        \n",
    "        m_media = sample_md.modules.create('media')\n",
    "        m_media.definition = media_md.persistentIdentity\n",
    "\n",
    "        # Create the attachment data as a csv\n",
    "        rna_series = row[[\"RNA_exp_1\", \"RNA_exp_2\", \"RNA_exp_ave\"]]\n",
    "        rna_df = pd.DataFrame([rna_series])  \n",
    "        rna_df.to_csv(os.path.join(attachments_path, attachment_file_name), index=False)\n",
    "        \n",
    "        # exp_attachment = sbol2.Attachment(exp_label)\n",
    "        # exp_attachment.name = f'fragmentation expression at rLP5 for sample {i}'\n",
    "        # exp_attachment.description = 'CSV including the gene expression of the sequence: RNA_exp1, RNA_exp2, and its average.'\n",
    "        # exp_attachment.source = 'CSV_LINK_HERE' # update when added attachment to SBOL collection\n",
    "        # exp_attachment.format = 'https://identifiers.org/edam/format_3752'\n",
    "        # doc.addAttachment(exp_attachment)\n",
    "        \n",
    "        # Experiment and Measurement Data\n",
    "        exp = sbol2.ExperimentalData(exp_label)\n",
    "        exp.wasDerivedFrom = sample_md.persistentIdentity\n",
    "        doc.add(exp)\n",
    "\n",
    "        exp_data_labels.append(exp_label)\n",
    "        attachment_file_names.append(attachment_file_name)\n",
    "\n",
    "        if (i == 5):\n",
    "            break\n",
    "            \n",
    "    report = doc.validate()\n",
    "    if (report == 'Valid.'):\n",
    "        doc.write('promoters.xml')\n",
    "    else:\n",
    "        print(report)\n",
    "    return exp_data_labels, attachment_file_names, doc, chassis_label_MD\n",
    "        \n",
    "def partshop_attach_exp_data(synbio_username, collection_name, file_names, email, password, exp_data_labels, version=1):\n",
    "    shop = sbol2.PartShop(\"https://synbiohub.org\")\n",
    "    print(shop.login(email, password))\n",
    "    exp_labels = [\"ExperimentalData_\" + label for label in exp_data_labels]  \n",
    "    for label, file_name in zip(exp_labels, file_names):\n",
    "        path = os.path.join(attachments_path, file_name)\n",
    "        attachment_uri = f\"https://synbiohub.org/user/{synbio_username}/{collection_name}/{label}/{version}\"\n",
    "        shop.attachFile(attachment_uri, path)\n",
    "\n",
    "def partshop_attach_genome_to_md(synbio_username, collection_name, file_path, email, password, chassis_label, version=1):\n",
    "    shop = sbol2.PartShop(\"https://synbiohub.org\")\n",
    "    print(shop.login(email, password))\n",
    "\n",
    "    label = \"ModuleDefinition_\" + chassis_label\n",
    "    attachment_uri = f\"https://synbiohub.org/user/{synbio_username}/{collection_name}/{label}/{version}\"\n",
    "\n",
    "    shop.attachFile(attachment_uri, file_path)\n",
    "\n",
    "def create_synbio_collection(email, password, file_path, id, name, description, version='1'):\n",
    "    response = requests.post(\n",
    "        \"https://synbiohub.org/login\",\n",
    "        headers={\"Accept\": \"text/plain\"},\n",
    "        data={\"email\": email, \"password\": password}\n",
    "    )\n",
    "        \n",
    "    if response.ok:\n",
    "        token = response.text.strip() # theres a whitespace before the token for some reason\n",
    "        response = requests.post(\n",
    "        'https://synbiohub.org/submit',\n",
    "        headers={\n",
    "            'X-authorization': token,\n",
    "            'Accept': 'text/plain'\n",
    "        },\n",
    "        files={\n",
    "        'files': open(file_path,'rb'),\n",
    "        },\n",
    "        data={\n",
    "            'id': id,\n",
    "            'version' : version,\n",
    "            'name' :  name,\n",
    "            'description' : description,\n",
    "            'citations' : '',\n",
    "            'overwrite_merge' : '0'\n",
    "        },\n",
    "    \n",
    "    )\n",
    "    else:\n",
    "        print(\"Login failed:\", response.status_code)\n",
    "        print(response.text)\n",
    "\n",
    "def partshop_pull(email, password, synbio_username, collection_name, file_path, version=1):\n",
    "    shop = sbol2.PartShop(\"https://synbiohub.org\")\n",
    "    doc = sbol2.Document()\n",
    "    shop.login(email, password)\n",
    "\n",
    "    collection_uri = f\"https://synbiohub.org/user/{synbio_username}/{collection_name}/{collection_name}_collection/{version}\"\n",
    "    s = shop.pull(collection_uri, doc)\n",
    "    \n",
    "    for obj in doc:\n",
    "        print(obj)   \n",
    "    \n",
    "    doc.write(file_path)\n",
    "\n",
    "def download_all_attachments(email, password, doc, file_path):\n",
    "    shop = sbol2.PartShop(\"https://synbiohub.org\")\n",
    "    shop.login(email, password)\n",
    "    for attachment in doc.attachments:\n",
    "        shop.downloadAttachment(attachment.identity, filepath=file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bf198d9-d05a-4a6a-9d2e-fad529266188",
   "metadata": {},
   "outputs": [],
   "source": [
    "ecoli_genome_file_name = \"E. coli.fasta\"\n",
    "env_email = os.getenv(\"SYNBIO_EMAIL\")\n",
    "env_password = os.getenv(\"SYNBIO_PASSWORD\")\n",
    "\n",
    "username = \"cywong\"\n",
    "output_name = 'ecolipromoters.xml'\n",
    "id = \"Ecolipromoterexpdata\"\n",
    "name = \"E coli promoter data exploration\"\n",
    "description = \"A collection containing the extracted E coli data from paper\"\n",
    "sbol_file_name = output_name\n",
    "imported_sbol_file_name = \"promoters_import.xml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0de59a6b-c443-4788-a8f0-0d1235fab7e1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'create_sbol_doc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m exp_labels, attachment_file_names, doc, chassis_label \u001b[38;5;241m=\u001b[39m create_sbol_doc(df, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(sbol_path, output_name))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'create_sbol_doc' is not defined"
     ]
    }
   ],
   "source": [
    "exp_labels, attachment_file_names, doc, chassis_label = create_sbol_doc(df, os.path.join(sbol_path, output_name))       \n",
    "#create_synbio_collection(env_email, env_password, os.path.join(sbol_path, sbol_file_name), id, name, description)\n",
    "#partshop_attach_exp_data(username, id, attachment_file_names, env_email, env_password, exp_labels)\n",
    "#partshop_attach_genome_to_md(username, id, os.path.join(attachments_path, ecoli_genome_file_name), env_email, env_password, chassis_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c0cfd1-3334-4808-bb9e-2b20ef3c35b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "partshop_pull(env_email, env_password, username, id, os.path.join(downloaded_sbol_path, imported_sbol_file_name), version=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7df4bf5-981f-44ff-8a1a-573dd9996bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download_all_attachments(env_email, env_password, doc, pulled_attachments_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b8bd54-df14-4eba-a637-138bb295fcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = sbol2.Document()\n",
    "doc.read(os.path.join(downloaded_sbol_path, imported_sbol_file_name))\n",
    "\n",
    "df_new = pd.DataFrame()\n",
    "\n",
    "for exp in doc.experimentalData:\n",
    "    attachment = doc.get(exp.attachments[0])\n",
    "    sample_design = doc.get(exp.wasDerivedFrom[0])\n",
    "\n",
    "    # may need to be changed\n",
    "    strain = doc.get(sample_design).modules[0] if 'strain' in doc.get(sample_design).modules[0].identity else doc.get(sample_design).modules[1]\n",
    "\n",
    "    promoter =  doc.get(doc.get(strain.definition).functionalComponents[0].definition)\n",
    "    promoter_seq = doc.get(promoter.sequences[0]).elements\n",
    "    \n",
    "    df1 = pd.read_csv(os.path.join(pulled_attachments_path, attachment.name))\n",
    "    df1['Sequence'] = promoter_seq\n",
    "    df_new = pd.concat([df_new, df1], ignore_index=True)\n",
    "\n",
    "\n",
    "# for mod in doc.moduleDefinitions:\n",
    "#     print(\"Mod: \", mod.identity)\n",
    "# for attachment in doc.attachments:\n",
    "#     print(\"Attachment: \", attachment.source) \n",
    "    \n",
    "# for seq in doc.sequences:\n",
    "#     print(\"Seq:\", seq.elements) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8fc6333-4cb4-4abf-a9a8-bdb50c2a4051",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "current_dir = os.path.abspath('')\n",
    "\n",
    "data_path = os.path.join(current_dir, '..', 'data')\n",
    "\n",
    "df3 = pd.read_csv(os.path.join(data_path, \"endo_scramble_expression_formatted_std.txt\"), delimiter= \"\\t\")\n",
    "df3\n",
    "\n",
    "# Positive promoter controls are synthetic thus have no actual location data. This refers to the 2,000 out of the 17,000 TSS they found, scrambled them to find the functional sites, including controls, scrambled variants, and unscrambled\n",
    "    # name: ID represented as {[TSS name][genomic position][strand + the scrambled region]}\n",
    "    # tss_name: Name of the original (unscrambled) transcription start site (TSS) this variant is derived from.\n",
    "    # tss_position: Start genome coordinate of the TSS.\n",
    "    # strand: Strand direction\n",
    "    # scramble_start: Start of the 10 bp scrambled window relative to var_left and var_right\n",
    "    # scramble_end: End of the scrambled window relative to var_left and var_right\n",
    "    # var_left: Genomic coordinate of the start (5' end) of the full 150 bp promoter variant.\n",
    "    # var_right: Genomic coordinate of the end (3' end) of the full 150 bp promoter variant.\n",
    "    # scramble_start_pos: Genomic coordinate where the scrambled 10 bp region begins.\n",
    "    # scramble_end_pos: Genomic coordinate where the scrambled 10 bp region ends.\n",
    "    # scramble_pos_rel_tss: Position of the scrambled region relative to the TSS.\n",
    "    # variant: The sequence\n",
    "    # RNA_exp_sum_1_1: Sum of RNA expression counts for barcodes in replicate 1, technical replicate 1 (normalized by DNA?)\n",
    "    # RNA_exp_sum_1_2: Sum of RNA expression counts for barcodes in replicate 1, technical replicate 2.\n",
    "    # RNA_exp_sum_2_1: Sum of RNA expression counts for barcodes in replicate 2, technical replicate 1.\n",
    "    # RNA_exp_sum_2_2: Sum of RNA expression counts for barcodes in replicate 2, technical replicate 2.\n",
    "    # RNA_exp_sum_1: Sum of RNA expression across both technical replicates for biological replicate 1.\n",
    "    # RNA_exp_sum_2: Sum of RNA expression across both technical replicates for biological replicate 2.\n",
    "    # RNA_exp_sum_ave: (RNA_exp_sum_1 + RNA_exp_sum_2)/2)\n",
    "    # DNA_1: DNA-seq count in biological replicate 1 (how much of this fragment was integrated).\n",
    "    # DNA_2: DNA-seq count in biological replicate 2\n",
    "    # DNA_ave: (DNA_1 + DNA_2) / 2\n",
    "    # expn_med: Median RNA expression across all barcodes, normalized by DNA abundance (RNA/DNA).\n",
    "    # num_barcodes_integrated: Number of barcodes actually integrated and measured in DNA/RNA-seq.\n",
    "    # category: Label indicating whether this is scrambled, unscrambled, negative, positive control\n",
    "    # unscrambled_exp: Measured promoter activity of the wild-type (unscrambled) version of this promoter.\n",
    "    # relative_exp: (expn_med / unscrambled_exp)\n",
    "\n",
    "df4 = pd.read_csv(os.path.join(data_path, \"fLP3_Endo2_lb_expression_formatted_std.txt\"), delimiter=\"\\t\")\n",
    "df4\n",
    "# Expression summary for the TSS promoter library (17,635 reported TSSs) in LB, integrated at the fLP3 site\n",
    "    # name: Unique identifier for the promoter variant in the format [TSS_name,TSS_position,strand]\n",
    "    # tss_name: The named identifier for the transcription start site (TSS), such as from RegulonDB or Storz \n",
    "    # tss_position: Genomic coordinate of the TSS — the position where transcription starts.\n",
    "    # strand: Strand direction\n",
    "    # start: Absolute genomic coordinate where the 150 bp promoter fragment starts.\n",
    "    # end: Absolute genomic coordinate where the 150 bp promoter fragment ends.\n",
    "    # variant: The promoter sequence\n",
    "    # RNA_exp_sum_1_1: Sum of RNA-seq counts for this variant in biological replicate 1, technical replicate 1.\n",
    "    # RNA_exp_sum_1_2: Sum of RNA-seq counts in biological replicate 1, technical replicate 2.\n",
    "    # RNA_exp_sum_2_1: Sum of RNA-seq counts in biological replicate 2, technical replicate 1.\n",
    "    # RNA_exp_sum_2_2: Sum of RNA-seq counts in biological replicate 2, technical replicate 2.\n",
    "    # RNA_exp_sum_1: Total RNA expression for biological replicate 1 \n",
    "    # RNA_exp_sum_2: Total RNA expression for biological replicate 2 \n",
    "    # RNA_exp_sum_ave: Average of RNA expression across the two biological replicates.\n",
    "    # DNA_1: DNA-seq count in biological replicate 1 \n",
    "    # DNA_2: DNA-seq count in biological replicate 2.\n",
    "    # DNA_ave: Average of DNA_1 and DNA_2\n",
    "    # expn_med: Median RNA/DNA expression across barcodes \n",
    "    # num_barcodes_integrated: Number of barcodes that were actually integrated and sequenced.\n",
    "    # category: Type of promoter: ['tss', 'neg_control', 'pos_control']\n",
    "    # active: Binary classification — \"active\" or \"inactive\" promoter based on expn_med\n",
    "\n",
    "\n",
    "df5 = pd.read_csv(os.path.join(data_path,\"peak_tile_expression_formatted_std.txt\"), delimiter=\"\\t\")\n",
    "df5\n",
    "# The authors first used sheared genomic fragments (~200–300 bp) to find candidate promoter regions, and then designed a tiling oligo library: ~150 bp sequences overlapping by 10 bp Spanning all ~3,500 candidate promoter region, located at LB and nth-ydgR intergenic locus \n",
    "    # variant: The 150 bp DNA sequence (oligo) used in the MPRA tile\n",
    "    # name: Identifier for the tile, usually formatted as “peak_start_peak_end_strand_posStart-posEnd”.\n",
    "    # peak_start: Genomic start coordinate of the candidate promoter region (the whole peak region).\n",
    "    # peak_end: Genomic end coordinate of the candidate promoter region.\n",
    "    # strand: Strand direction\n",
    "    # tile_start: Start of this 150 bp oligo relative to the peak\n",
    "    # tile_end: End of the tile \n",
    "    # RNA_exp_sum_1_1: Sum of RNA reads for this tile in biological replicate 1, technical replicate 1\n",
    "    # RNA_exp_sum_1_2: Same as above, but technical replicate 2\n",
    "    # RNA_exp_sum_2_1: Sum of RNA reads in biological replicate 2, technical replicate 1.\n",
    "    # RNA_exp_sum_2_2: Same as above, but technical replicate 2.\n",
    "    # RNA_exp_sum_1: Total RNA expression in biological replicate 1\n",
    "    # RNA_exp_sum_2: Total RNA expression in biological replicate 2 \n",
    "    # RNA_exp_sum_ave: Average RNA expression across the two biological replicates.\n",
    "    # DNA_1: DNA read count in biological replicate 1\n",
    "    # DNA_2: DNA read count in biological replicate 2.\n",
    "    # DNA_ave: Average of DNA_1 and DNA_2\n",
    "    # expn_med: Median RNA/DNA expression across all barcodes for this tile \n",
    "    # num_barcodes_integrated: Number of barcodes that were successfully integrated and sequenced.\n",
    "    # category: ['tile', 'neg_control', 'random', 'pos_control']\n",
    "    # peak_length: Length (in bp) of the peak region from which this tile was derived (e.g., 362 bp).\n",
    "    # tile_start_relative: Position of the tile’s start within the peak (tile_start / peak_length)\n",
    "    # start: Genomic start coordinate of the tile (based on tile_start + peak_start).\n",
    "    # end: Genomic end coordinate of the tile.\n",
    "    # active: Binary label indicating if the tile is active or inactive\n",
    "\n",
    "df6 = pd.read_csv(os.path.join(data_path,\"rLP5_Endo2_lb_expression_formatted_std.txt\"), delimiter=\"\\t\")\n",
    "df6\n",
    "# Expression summary for the TSS promoter library (17,635 reported TSSs) in LB, integrated at the rLP5 site\n",
    "\n",
    "df7 = pd.read_csv(os.path.join(data_path, \"rLP6_Endo2_lb_expression_formatted_std.txt\"), delimiter=\"\\t\")\n",
    "df7\n",
    "# Expression summary for the TSS promoter library (17,635 reported TSSs) in LB, integrated at the rLP6 site\n",
    "\n",
    "        \n",
    "def post_all_attachments(synbio_username, collection_name, file_names, email, password, exp_data_labels, version=1):\n",
    "    sbh_url = \"https://synbiohub.org/login\" \n",
    "    sbh_email = email\n",
    "    sbh_password = password\n",
    "    \n",
    "    response = requests.post(\n",
    "        sbh_url,\n",
    "        headers={\"Accept\": \"text/plain\"},\n",
    "        data={\"email\": sbh_email, \"password\": sbh_password}\n",
    "    )\n",
    "\n",
    "    if response.ok:\n",
    "        token = response.text.strip() # theres a whitespace before the token for some reason\n",
    "        print(token)\n",
    "        exp_labels = [\"ExperimentalData_\" + label for label in exp_data_labels]  \n",
    "        \n",
    "        for label, file_name in zip(exp_labels, file_names):\n",
    "            attachment_uri = f\"https://synbiohub.org/user/{synbio_username}/{collection_name}/{label}/{version}/attach\"\n",
    "            print(attachment_uri)\n",
    "            with open(file_name, 'rb') as f:\n",
    "                response = requests.post(\n",
    "                    attachment_uri,\n",
    "                    headers={\n",
    "                        'X-authorization': token,\n",
    "                        'Accept': 'text/plain'\n",
    "                    },\n",
    "                    files={'file': f}\n",
    "                )\n",
    "    \n",
    "            \n",
    "            print(\"Status Code:\", response.status_code)\n",
    "            print(\"Response Body:\", response.text)\n",
    "    else:\n",
    "        print(\"Login failed:\", response.status_code)\n",
    "        print(response.text)\n",
    "\n",
    "def post_attachment(attachment_uri, file_path, email, password):\n",
    "    sbh_url = \"https://synbiohub.org/login\" \n",
    "    sbh_email = email\n",
    "    sbh_password = password\n",
    "    \n",
    "    response = requests.post(\n",
    "        sbh_url,\n",
    "        headers={\"Accept\": \"text/plain\"},\n",
    "        data={\"email\": sbh_email, \"password\": sbh_password}\n",
    "    )\n",
    "    \n",
    "    if response.ok:\n",
    "        token = response.text.strip() # theres a whitespace before the token for some reason\n",
    "        print(token)\n",
    "\n",
    "        with open(file_path, 'rb') as f:\n",
    "            response = requests.post(\n",
    "                attachment_uri,\n",
    "                headers={\n",
    "                    'X-authorization': token,\n",
    "                    'Accept': 'text/plain'\n",
    "                },\n",
    "                files={'file': f}\n",
    "            )\n",
    "        \n",
    "        print(\"Status Code:\", response.status_code)\n",
    "        print(\"Response Body:\", response.text)\n",
    "    \n",
    "    else:\n",
    "        print(\"Login failed:\", response.status_code)\n",
    "        print(response.text)\n",
    "\n",
    "#df1 and df2\n",
    "\n",
    "# fragment: 150–300 bp genomic DNA sequence that were randomly sheared and barcoded\n",
    "# RNA_exp_1: RNA expression level (replicate 1) – normalized measurement of transcript abundance for this fragment in the first RNA-Seq replicate. (normalized by DNA)\n",
    "# RNA_exp_2: RNA expression level (replicate 2) – same as above, but from a second biological replicate.\n",
    "# RNA_exp_ave: Average RNA expression – mean of RNA_exp_1 and RNA_exp_2\n",
    "# DNA_sum_1: DNA integration abundance (replicate 1) – quantifies how much of this DNA fragment (or barcode) was actually integrated in the first DNA-Seq sample. \n",
    "# DNA_sum_2: DNA integration abundance (replicate 2) – same as above, from the second replicate.\n",
    "# DNA_ave: Average DNA integration level – mean of DNA_sum_1 and DNA_sum_2\n",
    "# num_integrated_barcodes: Number of barcodes that were integrated into the genome for this fragment. Typically should match num_mapped_barcodes unless there are sequencing/integration issues.\n",
    "# start: Start coordinate (in bp) in the E. coli MG1655 reference genome (U00096.2). \n",
    "# end: End coordinate in the E. coli MG1655 reference genome (U00096.2). \n",
    "# strand: DNA strand orientation (+ or -) \n",
    "# variation: Standard deviation or variability in RNA measurement across barcode replicates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f37a6716-fc16-4896-aa19-c85b3902f471",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, random\n",
    "np.random.seed(1)\n",
    "random.seed(1)\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split  # sklearn >= 0.18\n",
    "import sys\n",
    "import argparse\n",
    "num_epochs = 100\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "\n",
    "# \tparser = argparse.ArgumentParser()\n",
    "# \tparser.add_argument('train', help='''pre-defined training set, \n",
    "# \t\tone column sequence, one column expression. Tab-separated''')\n",
    "# \tparser.add_argument('test', help='pre-defined test set')\n",
    "# \tparser.add_argument('seq_length', type=int, help='length of input sequences')\n",
    "# \tparser.add_argument('num_layers', type=int, help='number of convolutional layers')\n",
    "# \tparser.add_argument('min_filter', type=int, help='minimum number of filters')\n",
    "# \tparser.add_argument('max_filter', type=int, help='maximum number of filters')\n",
    "# \tparser.add_argument('validation_fraction', type=float)\n",
    "# \tparser.add_argument('num_trials', type=int, \n",
    "# \t\thelp='number of hyperparameter trials')\n",
    "# \tparser.add_argument('prefix', help='output prefix for saved model files')\n",
    "# \tparser.add_argument('--validation', help='Optional pre-defined validation set')\n",
    "\t\n",
    "# \targs = parser.parse_args()\n",
    "\n",
    "# \t# load in pre-defined splits\n",
    "# \tseq_length = args.seq_length\n",
    "# \tprint(\"loading training set...\")\n",
    "# \tX_train, y_train = process_seqs(args.train, seq_length)\n",
    "# \tprint(\"loading test set...\")\n",
    "# \tX_test, y_test = process_seqs(args.test, seq_length)\n",
    "\n",
    "\t\n",
    "# \tnum_layers = args.num_layers\n",
    "# \tmin_filter = args.min_filter\n",
    "# \tmax_filter = args.max_filter\n",
    "# \tvalidation_fraction = args.validation_fraction\n",
    "# \tnum_hyperparameter_trials = args.num_trials\n",
    "# \tprefix = args.prefix\n",
    "\n",
    "# \tif args.validation:\n",
    "# \t\tX_valid, y_valid = process_seqs(args.validation, seq_length)\n",
    "# \telse:\n",
    "# \t\t# split training into validation set\n",
    "# \t\tX_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, \n",
    "# \t\t\ttest_size=validation_fraction)\n",
    "\n",
    "\n",
    "# \tprint('Starting hyperparameter search...')\n",
    "# \tmin_layer = 1\n",
    "# \tmax_layer = 4\n",
    "# \tmin_conv_width = 15\n",
    "# \tmax_conv_width = 20\n",
    "# \tmin_dropout = 0.1\n",
    "# \tmax_dropout = 0.9\n",
    "\n",
    "# \tfixed_hyperparameters = {'seq_length': seq_length, 'num_epochs': num_epochs}\n",
    "# \tgrid = {'num_filters': ((min_filter, max_filter),), 'pool_width': (5, 40),\n",
    "# \t        'conv_width': ((min_conv_width, max_conv_width),), \n",
    "# \t        'dropout': (min_dropout, max_dropout)}\n",
    "\n",
    "# \t# number of convolutional layers        \n",
    "# \tprint(\"Number of convolutional layers: \", num_layers)\n",
    "# \tfilters = tuple([(min_filter, max_filter)] * num_layers)\n",
    "# \tconv_widths = tuple([(min_conv_width, max_conv_width)] * num_layers)\n",
    "# \tgrid.update({'num_filters': filters, 'conv_width': conv_widths})\n",
    "\n",
    "# \t# Backend is RandomSearch; if using Python 2, can also specify MOESearch\n",
    "# \t# (requires separate installation)\n",
    "# \tsearcher = HyperparameterSearcher(SequenceDNN, fixed_hyperparameters, grid, \n",
    "# \t\tX_train, y_train, validation_data=(X_valid, y_valid), backend=RandomSearch)\n",
    "# \tsearcher.search(num_hyperparameter_trials)\n",
    "# \tprint('Best hyperparameters: {}'.format(searcher.best_hyperparameters))\n",
    "# \tmodel = searcher.best_model\n",
    "\t\n",
    "# \t# print test results\n",
    "# \tprint('Test results: {}'.format(model.score(X_test, y_test)))\n",
    "\t\n",
    "# \t# save model\n",
    "# \tmodel.save(prefix)\n",
    "\t\n",
    "# \t# print predictions\n",
    "# \tpredictions = model.predict(X_test)\n",
    "# \ttest_sequences = [line.split('\\t')[0] for line in open(args.test)]\n",
    "\t\n",
    "# \twith open(prefix + '_predictions.txt', 'w') as outfile:\n",
    "# \t\tfor i in range(len(predictions)):\n",
    "# \t\t\toutfile.write(\n",
    "# \t\t\t\ttest_sequences[i] + '\\t' + \n",
    "# \t\t\t\tstr(float(predictions[i])) + '\\t' + \n",
    "# \t\t\t\tstr(float(y_test[i])) + '\\n')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e78b5634-1dfe-4f87-b7a6-a5de6546d1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode_modern(sequences):\n",
    "    sequences_array = np.array([[char for char in seq] for seq in sequences])\n",
    "    sequence_length = sequences_array.shape[1]\n",
    "    num_samples = sequences_array.shape[0]\n",
    "\n",
    "    defined_categories = ['A', 'C', 'G', 'T', 'N']\n",
    "    ohe = OneHotEncoder(sparse=False,\n",
    "                        categories=[defined_categories] * sequence_length,\n",
    "                        dtype=np.float32) # Still keep this as float32\n",
    "\n",
    "    one_hot_encoding_flat = ohe.fit_transform(sequences_array)\n",
    "\n",
    "    # Reshape to (num_samples, sequence_length, num_bases)\n",
    "    one_hot_encoding_reshaped = one_hot_encoding_flat.reshape(\n",
    "        num_samples, sequence_length, len(defined_categories))\n",
    "\n",
    "    # Add the 'height' dimension (which will be 1 for a 1D sequence interpreted as 2D)\n",
    "    # and ensure channels are last: (N, H, W, C) -> (num_samples, 1, sequence_length, 4)\n",
    "    # This becomes (num_samples, height=1, width=sequence_length, channels=4)\n",
    "    # Keras Conv2D with channels_last expects (batch, rows, cols, channels)\n",
    "    # For sequence, if you treat rows=1, cols=seq_length, channels=4\n",
    "    # then it should be (N, 1, seq_length, 4)\n",
    "    final_one_hot_encoding = one_hot_encoding_reshaped[:, np.newaxis, :, :] # (N, 1, L, 4)\n",
    "    # If your Conv2D is truly 2D over a sequence, this shape is fine.\n",
    "    # If it's more like 1D conv, you might need (N, L, C) and then a Reshape for Conv2D.\n",
    "    # Given your current Conv2D setup and input_shape:\n",
    "    # Conv2D(..., input_shape=(1, 4, seq_length), data_format='channels_first')\n",
    "    # You want Conv2D(..., input_shape=(1, seq_length, 4), data_format='channels_last')\n",
    "\n",
    "    # Let's target: (num_samples, height, width, channels)\n",
    "    # For your 4-channel DNA, it's (num_samples, 1, seq_length, 4) or just (num_samples, seq_length, 4) if it's a \"1D\" Conv2D.\n",
    "    # Given your `conv_height=4` for the first layer, it seems you want the 4 bases as height.\n",
    "    # This implies: (N, 4, seq_length, 1) or (N, seq_length, 4, 1) as input, if you want to use channels_last.\n",
    "    # The original code's input_shape=(1, 4, seq_length) and conv_height=4 suggests NCHW (N, C, H, W) where C=1.\n",
    "    # If C=1, then it's (N, 1, 4, seq_length) where 4 is the 'height' and seq_length is 'width'.\n",
    "    # For channels_last, that would be (N, 4, seq_length, 1).\n",
    "\n",
    "    # So, from (num_samples, sequence_length, len(defined_categories))\n",
    "    # you want (num_samples, Height=4, Width=sequence_length, Channels=1)\n",
    "\n",
    "    # Transpose to get channels last if your original was (N, C, H, W)\n",
    "    # From (num_samples, sequence_length, 4) where 4 are the one-hot channels\n",
    "    # We need (num_samples, H, W, C) where H=4, W=seq_length, C=1\n",
    "    # This is a bit tricky given your initial Conv2D setup.\n",
    "\n",
    "    # Let's simplify the one-hot encoding output:\n",
    "    # Output directly (num_samples, seq_length, 4) as (batch, steps, features)\n",
    "    # Then we'll reshape for Conv2D inside SequenceDNN if needed.\n",
    "    # (N, L, 4) where L is sequence_length, 4 is features (A,C,G,T).\n",
    "    return one_hot_encoding_reshaped # (N, L, 4)\n",
    "    return final_one_hot_encoding\n",
    "\n",
    "def pad_sequence(seq, max_length):\n",
    "\tif len(seq) > max_length:\n",
    "\t\tdiff = len(seq) - max_length\n",
    "\t\ttrim_length = int(diff / 2)\n",
    "\t\tseq = seq[trim_length : -(trim_length + diff%2)]\n",
    "\telse:\n",
    "\t\tseq = seq.center(max_length, 'N')\n",
    "\treturn seq\n",
    "\n",
    "def process_seqs(df, seq_length):\n",
    "\tpadded_seqs = [pad_sequence(x, seq_length) for x in df['variant']]\n",
    "\tX = one_hot_encode_modern(np.array(padded_seqs))\n",
    "\ty = np.array(df['expn_med_fitted_scaled'], dtype=np.float32)\n",
    "\treturn X, y\n",
    "\n",
    "\n",
    "def split_data_by_peak(data_df):\n",
    "    np.random.seed(123)\n",
    "\n",
    "    # 1. Create 'peak_name' column (if not already present)\n",
    "    # Assuming peak_start and peak_end are numeric, convert to string for concatenation\n",
    "    data_df['peak_name'] = data_df['peak_start'].astype(str) + 'to' + data_df['peak_end'].astype(str)\n",
    "\n",
    "    # 2. Get unique peak names\n",
    "    peak_names = data_df['peak_name'].unique()\n",
    "\n",
    "    # 3. Randomly sample train peak names (90%)\n",
    "    train_size = int(0.90 * len(peak_names))\n",
    "    train_peak_names = np.random.choice(peak_names, size=train_size, replace=False)\n",
    "\n",
    "    # 4. Determine test peak names\n",
    "    test_peak_names = np.array([p for p in peak_names if p not in train_peak_names])\n",
    "\n",
    "\n",
    "    data_train = data_df[data_df['peak_name'].isin(train_peak_names)].copy()\n",
    "    data_test = data_df[data_df['peak_name'].isin(test_peak_names)].copy()\n",
    "\n",
    "    return data_train, data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "734071ac-e41c-4789-92e6-37a8bc4fb65f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function BaseSession._Callable.__del__ at 0x000001B11311CAE8>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Sai\\anaconda3\\envs\\tf1.0\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1455, in __del__\n",
      "    self._session._session, self._handle, status)\n",
      "  File \"C:\\Users\\Sai\\anaconda3\\envs\\tf1.0\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\", line 528, in __exit__\n",
      "    c_api.TF_GetCode(self.status.status))\n",
      "tensorflow.python.framework.errors_impl.InvalidArgumentError: No such callable handle: 0\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b1d5c26d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0., 1., 0.],\n",
       "        [0., 1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 1., 0.],\n",
       "        [0., 1., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 1., 0.],\n",
       "        [0., 1., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 1., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1.]],\n",
       "\n",
       "       [[0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 1.],\n",
       "        ...,\n",
       "        [0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 1.]],\n",
       "\n",
       "       [[0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 1.],\n",
       "        ...,\n",
       "        [0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 1.]]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ffd4aa-c904-47ba-b18e-6f94fcdd2840",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "import numpy as np\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "import tempfile\n",
    "# matplotlib.use('pdf')\n",
    "# import matplotlib.pyplot as plt\n",
    "from abc import abstractmethod, ABCMeta\n",
    "from sklearn.tree import DecisionTreeClassifier as scikit_DecisionTree\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "class Model(object):\n",
    "    __metaclass__ = ABCMeta\n",
    "\n",
    "    @abstractmethod\n",
    "    def __init__(self, **hyperparameters):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def train(self, X, y, validation_data):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(self, X):\n",
    "        pass\n",
    "\n",
    "    def test(self, X, y):\n",
    "        return self.evaluate(X, y)\n",
    "        # return ClassificationResult(y, self.predict(X))\n",
    "\n",
    "    def score(self, X, y):\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "class DecisionTree(Model):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.classifier = scikit_DecisionTree()\n",
    "\n",
    "    def train(self, X, y, validation_data=None):\n",
    "        self.classifier.fit(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = np.asarray(self.classifier.predict_proba(X))[..., 1]\n",
    "        if len(predictions.shape) == 2:  # multitask\n",
    "            predictions = predictions.T\n",
    "        else:  # single-task\n",
    "            predictions = np.expand_dims(predictions, 1)\n",
    "        return predictions\n",
    "    \n",
    "\n",
    "class RandomForestRegression(DecisionTree):\n",
    "    def __init__(self):\n",
    "        self.regressor = RandomForestRegressor(n_estimators=100)\n",
    "\n",
    "    def train(self, X, y, validation_data=None):\n",
    "        # X shape: n_samples, n_features\n",
    "        # y shape: n_samples\n",
    "        self.regressor.fit(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.regressor.predict(X)\n",
    "\n",
    "    def score(self, X, y):\n",
    "        predictions = np.squeeze(self.regressor.predict(X))\n",
    "        return np.corrcoef(predictions, y)[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d6c2477e-0820-44b3-9bdb-5a5240e9fd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode_modern(sequences):\n",
    "    sequences_array = np.array([[char for char in seq] for seq in sequences])\n",
    "    sequence_length = sequences_array.shape[1]\n",
    "    num_samples = sequences_array.shape[0]\n",
    "\n",
    "    defined_categories = ['A', 'C', 'G', 'T', 'N']\n",
    "    ohe = OneHotEncoder(sparse_output=False,\n",
    "                        categories=[defined_categories] * sequence_length,\n",
    "                        dtype=np.float32) # Still keep this as float32\n",
    "\n",
    "    return ohe.fit_transform(sequences_array)\n",
    "\n",
    "def pad_sequence(seq, max_length):\n",
    "\tif len(seq) > max_length:\n",
    "\t\tdiff = len(seq) - max_length\n",
    "\t\ttrim_length = int(diff / 2)\n",
    "\t\tseq = seq[trim_length : -(trim_length + diff%2)]\n",
    "\telse:\n",
    "\t\tseq = seq.center(max_length, 'N')\n",
    "\treturn seq\n",
    "\n",
    "def process_seqs(df, seq_length):\n",
    "\tpadded_seqs = [pad_sequence(x, seq_length) for x in df['variant']]\n",
    "\tX = one_hot_encode_modern(np.array(padded_seqs))\n",
    "\ty = np.array(df['expn_med_fitted_scaled'], dtype=np.float32)\n",
    "\treturn X, y\n",
    "\n",
    "\n",
    "def split_data_by_peak(data_df):\n",
    "    np.random.seed(123)\n",
    "\n",
    "    # 1. Create 'peak_name' column (if not already present)\n",
    "    # Assuming peak_start and peak_end are numeric, convert to string for concatenation\n",
    "    data_df['peak_name'] = data_df['peak_start'].astype(str) + 'to' + data_df['peak_end'].astype(str)\n",
    "\n",
    "    # 2. Get unique peak names\n",
    "    peak_names = data_df['peak_name'].unique()\n",
    "\n",
    "    # 3. Randomly sample train peak names (90%)\n",
    "    train_size = int(0.90 * len(peak_names))\n",
    "    train_peak_names = np.random.choice(peak_names, size=train_size, replace=False)\n",
    "\n",
    "    # 4. Determine test peak names\n",
    "    test_peak_names = np.array([p for p in peak_names if p not in train_peak_names])\n",
    "\n",
    "\n",
    "    data_train = data_df[data_df['peak_name'].isin(train_peak_names)].copy()\n",
    "    data_test = data_df[data_df['peak_name'].isin(test_peak_names)].copy()\n",
    "\n",
    "    return data_train, data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8372b0b1-6e21-44f8-9e00-45c3693325d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_df = df5\n",
    "train_df, test_df = split_data_by_peak(data_df.copy())\n",
    "\n",
    "X_train, y_train = process_seqs(train_df, 150)\n",
    "X_test, y_test = process_seqs(test_df, 150)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "88e0edab-aa2b-43a1-9fe6-b77411c4ed45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np, random\n",
    "np.random.seed(1)\n",
    "random.seed(1)\n",
    "from sklearn.model_selection import train_test_split  # sklearn >= 0.18\n",
    "import sys\n",
    "import argparse\n",
    "import time\n",
    "\n",
    "def train_RandomForest(X_train, X_test, y_train, y_test):\n",
    "    print(\"Running random forest regression...\")\n",
    "    model = RandomForestRegression()\n",
    "    model.train(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    \n",
    "    with open(\"outputs.txt\", 'w') as outfile:\n",
    "        for i in range(len(predictions)):\n",
    "            outfile.write(str(float(predictions[i])) + '\\t' +\n",
    "                      str(float(y_test[i])) + '\\n')\n",
    "    \n",
    "    score = model.score(X_test, y_test)\n",
    "    print(\"Score:\", score)\n",
    "    return model, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e104f186-43ed-4b50-9c54-e8377addc03a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running random forest regression...\n",
      "Score: 0.07955371559314629\n"
     ]
    }
   ],
   "source": [
    "model, predictions = train_RandomForest(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "482648da-d0b4-4116-840c-5ae4790d6f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 5.548276973597522\n",
      "Root Mean Square Error (RMSE): 2.3554780775030624\n",
      "R^2: -0.10019475191929983\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "r_squared = r2_score(y_test, predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"Root Mean Square Error (RMSE): {rmse}\")\n",
    "print(f\"R^2: {r_squared}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc449ca-b848-4dbb-8202-efb06b1ba463",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
