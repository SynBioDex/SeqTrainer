{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6169c8c4-6c46-41c1-9c84-3bff9d2e2c8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sbol2\n",
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "import zipfile\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "current_dir = os.path.abspath('')\n",
    "data_path = os.path.join(current_dir, '..', 'data')\n",
    "attachments_path = os.path.join(current_dir, '..', 'attachments')\n",
    "pulled_attachments_path = os.path.join(current_dir, '..', 'pulled_attachments')\n",
    "sbol_path = os.path.join(current_dir, '..', 'sbol_data')\n",
    "downloaded_sbol_path = os.path.join(current_dir, '..', 'downloaded_sbol')\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b881c7-b711-41ab-af99-5c3e0dcb7245",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(data_path, \"frag-rLP5_LB_expression.txt\"), delimiter=\" \")\n",
    "# Expression levels from genomic fragment MPRA (random 200–300 bp sheared fragments) in LB media\t\n",
    "df2 = pd.read_csv(os.path.join(data_path, \"frag-rLP5-M9_expression.txt\"), delimiter=\" \")\n",
    "# Expression levels from genomic fragment MPRA (random 200–300 bp sheared fragments) in M9 media at rLP5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2d2488-94ef-4002-a271-e0e8813de09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cffdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761f55c1-a68b-485f-826f-09a11af50b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sbol_doc(df, file_path, media=\"LB\"):\n",
    "    doc = sbol2.Document()\n",
    "    sbol2.setHomespace('http://github.com/cywlol/promoters')\n",
    "    doc.displayId = \"E_coli_promoters\"\n",
    "    # save labels for later when we do the attachments through api\n",
    "    exp_data_labels = []\n",
    "    attachment_file_names = []\n",
    "\n",
    "    media_label_MD = media\n",
    "    chassis_label_MD = \"E_coli_chassis\"\n",
    "\n",
    "    # define the media once, then have it as a reference for each promoter row\n",
    "    media_md = sbol2.ModuleDefinition(media_label_MD)\n",
    "    doc.addModuleDefinition(media_md)\n",
    "    media_md.addRole(\"http://identifiers.org/ncit/NCIT:C48164\") \n",
    "    \n",
    "    # define the chassis once, then have it as a reference for each promoter row\n",
    "    chassis_md = sbol2.ModuleDefinition(chassis_label_MD) \n",
    "    doc.addModuleDefinition(chassis_md)\n",
    "    chassis_md.addRole(\"http://identifiers.org/ncit/NCIT:C14419\")\n",
    "    \n",
    "    #attach genome \n",
    "    chassis_md.wasDerivedFrom = [\"https://www.ncbi.nlm.nih.gov/Taxonomy/Browser/wwwtax.cgi?id=511145\"]\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        fragment_seq = row[\"fragment\"]\n",
    "\n",
    "        # Establish the identities of each component, ensuring that each one is unique\n",
    "        promoter_label_CD = f\"promoter_{i}\"\n",
    "        promoter_seq_label = f\"promoter_seq_{i}\"\n",
    "        sample_design_label_CD = f\"sample_design_{i}\"\n",
    "        strain_label_MD = f\"strain_{i}\"\n",
    "        data_label = f\"exp_data_{i}\"\n",
    "        exp_label = f'sample_{i}_expression_data'\n",
    "        attachment_file_name = f'frag_{media}_exp_sample_{i}.csv'\n",
    "\n",
    "        #engr_region_label_CD = f\"engr_region_{i}\"\n",
    "        #location_promoter_annotation = f\"location_promoter_annotation_{i}\"\n",
    "        #location_promoter_label = f\"location_promoter_label_{i}\"\n",
    "        \n",
    "        # Promoter and Sequence \n",
    "        promoter_cd = sbol2.ComponentDefinition(promoter_label_CD, sbol2.BIOPAX_DNA)\n",
    "        promoter_cd.roles = [sbol2.SO_PROMOTER]\n",
    "        seq = sbol2.Sequence(promoter_seq_label, fragment_seq, sbol2.SBOL_ENCODING_IUPAC)\n",
    "        doc.addSequence(seq)\n",
    "        promoter_cd.sequences = [seq.persistentIdentity]\n",
    "        doc.addComponentDefinition(promoter_cd)\n",
    "\n",
    "        \n",
    "        # Add strain module definition\n",
    "        strain_md = sbol2.ModuleDefinition(strain_label_MD)\n",
    "        strain_md.addRole(\"http://identifiers.org/ncit/NCIT:C14419\")\n",
    "        doc.addModuleDefinition(strain_md)\n",
    "        strain_c1 = strain_md.modules.create('chassis')\n",
    "        strain_c1.definition = chassis_md.persistentIdentity\n",
    "        strain_c2 = strain_md.functionalComponents.create('promoter')\n",
    "        strain_c2.definition = promoter_cd.persistentIdentity\n",
    "    \n",
    "        #annotation = sbol2.SequenceAnnotation(\"promoter_location\")\n",
    "        #range = sbol2.Range(\"prange\", start, end)\n",
    "    \n",
    "        #if (strand == \"-\"):\n",
    "        #    range.orientation = sbol2.SBOL_ORIENTATION_REVERSE_COMPLEMENT\n",
    "                \n",
    "        #annotation.locations.add(range)\n",
    "        #promoter_cd.sequenceAnnotations.add(annotation)\n",
    "        '''\n",
    "        # Engineered Region \n",
    "        engineered_cd = sbol2.ComponentDefinition(engr_region_label_CD, sbol2.BIOPAX_DNA)\n",
    "        engineered_cd.roles = [\"https://identifiers.org/so/SO:0000804\"]\n",
    "        sub = engineered_cd.components.create('promoter')\n",
    "        sub.definition = promoter_cd.persistentIdentity\n",
    "        doc.addComponentDefinition(engineered_cd)\n",
    "        # No sequence?\n",
    "        '''\n",
    "        \n",
    "        #strain_c2 = strain_md.functionalComponents.create('engineered_region')\n",
    "        #strain_c2.definition = strain_md.persistentIdentity\n",
    "        \n",
    "        # Sample Design  \n",
    "        sample_md = sbol2.ModuleDefinition(sample_design_label_CD)\n",
    "        doc.addModuleDefinition(sample_md)\n",
    "        sample_md.addRole(\"http://identifiers.org/obo/OBI:0000073\")\n",
    "\n",
    "        m_strain = sample_md.modules.create('strain')\n",
    "        m_strain.definition = strain_md.persistentIdentity\n",
    "        \n",
    "        m_media = sample_md.modules.create('media')\n",
    "        m_media.definition = media_md.persistentIdentity\n",
    "\n",
    "        # Create the attachment data as a csv\n",
    "        rna_series = row[[\"RNA_exp_1\", \"RNA_exp_2\", \"RNA_exp_ave\"]]\n",
    "        rna_df = pd.DataFrame([rna_series])  \n",
    "        rna_df.to_csv(os.path.join(attachments_path, attachment_file_name), index=False)\n",
    "        \n",
    "        # exp_attachment = sbol2.Attachment(exp_label)\n",
    "        # exp_attachment.name = f'fragmentation expression at rLP5 for sample {i}'\n",
    "        # exp_attachment.description = 'CSV including the gene expression of the sequence: RNA_exp1, RNA_exp2, and its average.'\n",
    "        # exp_attachment.source = 'CSV_LINK_HERE' # update when added attachment to SBOL collection\n",
    "        # exp_attachment.format = 'https://identifiers.org/edam/format_3752'\n",
    "        # doc.addAttachment(exp_attachment)\n",
    "        \n",
    "        # Experiment and Measurement Data\n",
    "        exp = sbol2.ExperimentalData(exp_label)\n",
    "        exp.wasDerivedFrom = sample_md.persistentIdentity\n",
    "        doc.add(exp)\n",
    "\n",
    "        exp_data_labels.append(exp_label)\n",
    "        attachment_file_names.append(attachment_file_name)\n",
    "\n",
    "        if (i == 5):\n",
    "            break\n",
    "            \n",
    "    report = doc.validate()\n",
    "    if (report == 'Valid.'):\n",
    "        doc.write('promoters.xml')\n",
    "    else:\n",
    "        print(report)\n",
    "    return exp_data_labels, attachment_file_names, doc, chassis_label_MD\n",
    "        \n",
    "def partshop_attach_exp_data(synbio_username, collection_name, file_names, email, password, exp_data_labels, version=1):\n",
    "    shop = sbol2.PartShop(\"https://synbiohub.org\")\n",
    "    print(shop.login(email, password))\n",
    "    exp_labels = [\"ExperimentalData_\" + label for label in exp_data_labels]  \n",
    "    for label, file_name in zip(exp_labels, file_names):\n",
    "        path = os.path.join(attachments_path, file_name)\n",
    "        attachment_uri = f\"https://synbiohub.org/user/{synbio_username}/{collection_name}/{label}/{version}\"\n",
    "        shop.attachFile(attachment_uri, path)\n",
    "\n",
    "def partshop_attach_genome_to_md(synbio_username, collection_name, file_path, email, password, chassis_label, version=1):\n",
    "    shop = sbol2.PartShop(\"https://synbiohub.org\")\n",
    "    print(shop.login(email, password))\n",
    "\n",
    "    label = \"ModuleDefinition_\" + chassis_label\n",
    "    attachment_uri = f\"https://synbiohub.org/user/{synbio_username}/{collection_name}/{label}/{version}\"\n",
    "\n",
    "    shop.attachFile(attachment_uri, file_path)\n",
    "\n",
    "def create_synbio_collection(email, password, file_path, id, name, description, version='1'):\n",
    "    response = requests.post(\n",
    "        \"https://synbiohub.org/login\",\n",
    "        headers={\"Accept\": \"text/plain\"},\n",
    "        data={\"email\": email, \"password\": password}\n",
    "    )\n",
    "        \n",
    "    if response.ok:\n",
    "        token = response.text.strip() # theres a whitespace before the token for some reason\n",
    "        response = requests.post(\n",
    "        'https://synbiohub.org/submit',\n",
    "        headers={\n",
    "            'X-authorization': token,\n",
    "            'Accept': 'text/plain'\n",
    "        },\n",
    "        files={\n",
    "        'files': open(file_path,'rb'),\n",
    "        },\n",
    "        data={\n",
    "            'id': id,\n",
    "            'version' : version,\n",
    "            'name' :  name,\n",
    "            'description' : description,\n",
    "            'citations' : '',\n",
    "            'overwrite_merge' : '0'\n",
    "        },\n",
    "    \n",
    "    )\n",
    "    else:\n",
    "        print(\"Login failed:\", response.status_code)\n",
    "        print(response.text)\n",
    "\n",
    "def partshop_pull(email, password, synbio_username, collection_name, file_path, version=1):\n",
    "    shop = sbol2.PartShop(\"https://synbiohub.org\")\n",
    "    doc = sbol2.Document()\n",
    "    shop.login(email, password)\n",
    "\n",
    "    collection_uri = f\"https://synbiohub.org/user/{synbio_username}/{collection_name}/{collection_name}_collection/{version}\"\n",
    "    s = shop.pull(collection_uri, doc)\n",
    "    \n",
    "    for obj in doc:\n",
    "        print(obj)   \n",
    "    \n",
    "    doc.write(file_path)\n",
    "\n",
    "def download_all_attachments(email, password, doc, file_path):\n",
    "    shop = sbol2.PartShop(\"https://synbiohub.org\")\n",
    "    shop.login(email, password)\n",
    "    for attachment in doc.attachments:\n",
    "        shop.downloadAttachment(attachment.identity, filepath=file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bf198d9-d05a-4a6a-9d2e-fad529266188",
   "metadata": {},
   "outputs": [],
   "source": [
    "ecoli_genome_file_name = \"E. coli.fasta\"\n",
    "env_email = os.getenv(\"SYNBIO_EMAIL\")\n",
    "env_password = os.getenv(\"SYNBIO_PASSWORD\")\n",
    "\n",
    "username = \"cywong\"\n",
    "output_name = 'ecolipromoters.xml'\n",
    "id = \"Ecolipromoterexpdata\"\n",
    "name = \"E coli promoter data exploration\"\n",
    "description = \"A collection containing the extracted E coli data from paper\"\n",
    "sbol_file_name = output_name\n",
    "imported_sbol_file_name = \"promoters_import.xml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0de59a6b-c443-4788-a8f0-0d1235fab7e1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'create_sbol_doc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m exp_labels, attachment_file_names, doc, chassis_label \u001b[38;5;241m=\u001b[39m create_sbol_doc(df, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(sbol_path, output_name))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'create_sbol_doc' is not defined"
     ]
    }
   ],
   "source": [
    "exp_labels, attachment_file_names, doc, chassis_label = create_sbol_doc(df, os.path.join(sbol_path, output_name))       \n",
    "#create_synbio_collection(env_email, env_password, os.path.join(sbol_path, sbol_file_name), id, name, description)\n",
    "#partshop_attach_exp_data(username, id, attachment_file_names, env_email, env_password, exp_labels)\n",
    "#partshop_attach_genome_to_md(username, id, os.path.join(attachments_path, ecoli_genome_file_name), env_email, env_password, chassis_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c0cfd1-3334-4808-bb9e-2b20ef3c35b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "partshop_pull(env_email, env_password, username, id, os.path.join(downloaded_sbol_path, imported_sbol_file_name), version=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7df4bf5-981f-44ff-8a1a-573dd9996bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download_all_attachments(env_email, env_password, doc, pulled_attachments_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b8bd54-df14-4eba-a637-138bb295fcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = sbol2.Document()\n",
    "doc.read(os.path.join(downloaded_sbol_path, imported_sbol_file_name))\n",
    "\n",
    "df_new = pd.DataFrame()\n",
    "\n",
    "for exp in doc.experimentalData:\n",
    "    attachment = doc.get(exp.attachments[0])\n",
    "    sample_design = doc.get(exp.wasDerivedFrom[0])\n",
    "\n",
    "    # may need to be changed\n",
    "    strain = doc.get(sample_design).modules[0] if 'strain' in doc.get(sample_design).modules[0].identity else doc.get(sample_design).modules[1]\n",
    "\n",
    "    promoter =  doc.get(doc.get(strain.definition).functionalComponents[0].definition)\n",
    "    promoter_seq = doc.get(promoter.sequences[0]).elements\n",
    "    \n",
    "    df1 = pd.read_csv(os.path.join(pulled_attachments_path, attachment.name))\n",
    "    df1['Sequence'] = promoter_seq\n",
    "    df_new = pd.concat([df_new, df1], ignore_index=True)\n",
    "\n",
    "\n",
    "# for mod in doc.moduleDefinitions:\n",
    "#     print(\"Mod: \", mod.identity)\n",
    "# for attachment in doc.attachments:\n",
    "#     print(\"Attachment: \", attachment.source) \n",
    "    \n",
    "# for seq in doc.sequences:\n",
    "#     print(\"Seq:\", seq.elements) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8fc6333-4cb4-4abf-a9a8-bdb50c2a4051",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "current_dir = os.path.abspath('')\n",
    "\n",
    "data_path = os.path.join(current_dir, '..', 'data')\n",
    "\n",
    "df3 = pd.read_csv(os.path.join(data_path, \"endo_scramble_expression_formatted_std.txt\"), delimiter= \"\\t\")\n",
    "df3\n",
    "\n",
    "# Positive promoter controls are synthetic thus have no actual location data. This refers to the 2,000 out of the 17,000 TSS they found, scrambled them to find the functional sites, including controls, scrambled variants, and unscrambled\n",
    "    # name: ID represented as {[TSS name][genomic position][strand + the scrambled region]}\n",
    "    # tss_name: Name of the original (unscrambled) transcription start site (TSS) this variant is derived from.\n",
    "    # tss_position: Start genome coordinate of the TSS.\n",
    "    # strand: Strand direction\n",
    "    # scramble_start: Start of the 10 bp scrambled window relative to var_left and var_right\n",
    "    # scramble_end: End of the scrambled window relative to var_left and var_right\n",
    "    # var_left: Genomic coordinate of the start (5' end) of the full 150 bp promoter variant.\n",
    "    # var_right: Genomic coordinate of the end (3' end) of the full 150 bp promoter variant.\n",
    "    # scramble_start_pos: Genomic coordinate where the scrambled 10 bp region begins.\n",
    "    # scramble_end_pos: Genomic coordinate where the scrambled 10 bp region ends.\n",
    "    # scramble_pos_rel_tss: Position of the scrambled region relative to the TSS.\n",
    "    # variant: The sequence\n",
    "    # RNA_exp_sum_1_1: Sum of RNA expression counts for barcodes in replicate 1, technical replicate 1 (normalized by DNA?)\n",
    "    # RNA_exp_sum_1_2: Sum of RNA expression counts for barcodes in replicate 1, technical replicate 2.\n",
    "    # RNA_exp_sum_2_1: Sum of RNA expression counts for barcodes in replicate 2, technical replicate 1.\n",
    "    # RNA_exp_sum_2_2: Sum of RNA expression counts for barcodes in replicate 2, technical replicate 2.\n",
    "    # RNA_exp_sum_1: Sum of RNA expression across both technical replicates for biological replicate 1.\n",
    "    # RNA_exp_sum_2: Sum of RNA expression across both technical replicates for biological replicate 2.\n",
    "    # RNA_exp_sum_ave: (RNA_exp_sum_1 + RNA_exp_sum_2)/2)\n",
    "    # DNA_1: DNA-seq count in biological replicate 1 (how much of this fragment was integrated).\n",
    "    # DNA_2: DNA-seq count in biological replicate 2\n",
    "    # DNA_ave: (DNA_1 + DNA_2) / 2\n",
    "    # expn_med: Median RNA expression across all barcodes, normalized by DNA abundance (RNA/DNA).\n",
    "    # num_barcodes_integrated: Number of barcodes actually integrated and measured in DNA/RNA-seq.\n",
    "    # category: Label indicating whether this is scrambled, unscrambled, negative, positive control\n",
    "    # unscrambled_exp: Measured promoter activity of the wild-type (unscrambled) version of this promoter.\n",
    "    # relative_exp: (expn_med / unscrambled_exp)\n",
    "\n",
    "df4 = pd.read_csv(os.path.join(data_path, \"fLP3_Endo2_lb_expression_formatted_std.txt\"), delimiter=\"\\t\")\n",
    "df4\n",
    "# Expression summary for the TSS promoter library (17,635 reported TSSs) in LB, integrated at the fLP3 site\n",
    "    # name: Unique identifier for the promoter variant in the format [TSS_name,TSS_position,strand]\n",
    "    # tss_name: The named identifier for the transcription start site (TSS), such as from RegulonDB or Storz \n",
    "    # tss_position: Genomic coordinate of the TSS — the position where transcription starts.\n",
    "    # strand: Strand direction\n",
    "    # start: Absolute genomic coordinate where the 150 bp promoter fragment starts.\n",
    "    # end: Absolute genomic coordinate where the 150 bp promoter fragment ends.\n",
    "    # variant: The promoter sequence\n",
    "    # RNA_exp_sum_1_1: Sum of RNA-seq counts for this variant in biological replicate 1, technical replicate 1.\n",
    "    # RNA_exp_sum_1_2: Sum of RNA-seq counts in biological replicate 1, technical replicate 2.\n",
    "    # RNA_exp_sum_2_1: Sum of RNA-seq counts in biological replicate 2, technical replicate 1.\n",
    "    # RNA_exp_sum_2_2: Sum of RNA-seq counts in biological replicate 2, technical replicate 2.\n",
    "    # RNA_exp_sum_1: Total RNA expression for biological replicate 1 \n",
    "    # RNA_exp_sum_2: Total RNA expression for biological replicate 2 \n",
    "    # RNA_exp_sum_ave: Average of RNA expression across the two biological replicates.\n",
    "    # DNA_1: DNA-seq count in biological replicate 1 \n",
    "    # DNA_2: DNA-seq count in biological replicate 2.\n",
    "    # DNA_ave: Average of DNA_1 and DNA_2\n",
    "    # expn_med: Median RNA/DNA expression across barcodes \n",
    "    # num_barcodes_integrated: Number of barcodes that were actually integrated and sequenced.\n",
    "    # category: Type of promoter: ['tss', 'neg_control', 'pos_control']\n",
    "    # active: Binary classification — \"active\" or \"inactive\" promoter based on expn_med\n",
    "\n",
    "\n",
    "df5 = pd.read_csv(os.path.join(data_path,\"peak_tile_expression_formatted_std.txt\"), delimiter=\"\\t\")\n",
    "df5\n",
    "# The authors first used sheared genomic fragments (~200–300 bp) to find candidate promoter regions, and then designed a tiling oligo library: ~150 bp sequences overlapping by 10 bp Spanning all ~3,500 candidate promoter region, located at LB and nth-ydgR intergenic locus \n",
    "    # variant: The 150 bp DNA sequence (oligo) used in the MPRA tile\n",
    "    # name: Identifier for the tile, usually formatted as “peak_start_peak_end_strand_posStart-posEnd”.\n",
    "    # peak_start: Genomic start coordinate of the candidate promoter region (the whole peak region).\n",
    "    # peak_end: Genomic end coordinate of the candidate promoter region.\n",
    "    # strand: Strand direction\n",
    "    # tile_start: Start of this 150 bp oligo relative to the peak\n",
    "    # tile_end: End of the tile \n",
    "    # RNA_exp_sum_1_1: Sum of RNA reads for this tile in biological replicate 1, technical replicate 1\n",
    "    # RNA_exp_sum_1_2: Same as above, but technical replicate 2\n",
    "    # RNA_exp_sum_2_1: Sum of RNA reads in biological replicate 2, technical replicate 1.\n",
    "    # RNA_exp_sum_2_2: Same as above, but technical replicate 2.\n",
    "    # RNA_exp_sum_1: Total RNA expression in biological replicate 1\n",
    "    # RNA_exp_sum_2: Total RNA expression in biological replicate 2 \n",
    "    # RNA_exp_sum_ave: Average RNA expression across the two biological replicates.\n",
    "    # DNA_1: DNA read count in biological replicate 1\n",
    "    # DNA_2: DNA read count in biological replicate 2.\n",
    "    # DNA_ave: Average of DNA_1 and DNA_2\n",
    "    # expn_med: Median RNA/DNA expression across all barcodes for this tile \n",
    "    # num_barcodes_integrated: Number of barcodes that were successfully integrated and sequenced.\n",
    "    # category: ['tile', 'neg_control', 'random', 'pos_control']\n",
    "    # peak_length: Length (in bp) of the peak region from which this tile was derived (e.g., 362 bp).\n",
    "    # tile_start_relative: Position of the tile’s start within the peak (tile_start / peak_length)\n",
    "    # start: Genomic start coordinate of the tile (based on tile_start + peak_start).\n",
    "    # end: Genomic end coordinate of the tile.\n",
    "    # active: Binary label indicating if the tile is active or inactive\n",
    "\n",
    "df6 = pd.read_csv(os.path.join(data_path,\"rLP5_Endo2_lb_expression_formatted_std.txt\"), delimiter=\"\\t\")\n",
    "df6\n",
    "# Expression summary for the TSS promoter library (17,635 reported TSSs) in LB, integrated at the rLP5 site\n",
    "\n",
    "df7 = pd.read_csv(os.path.join(data_path, \"rLP6_Endo2_lb_expression_formatted_std.txt\"), delimiter=\"\\t\")\n",
    "df7\n",
    "# Expression summary for the TSS promoter library (17,635 reported TSSs) in LB, integrated at the rLP6 site\n",
    "\n",
    "        \n",
    "def post_all_attachments(synbio_username, collection_name, file_names, email, password, exp_data_labels, version=1):\n",
    "    sbh_url = \"https://synbiohub.org/login\" \n",
    "    sbh_email = email\n",
    "    sbh_password = password\n",
    "    \n",
    "    response = requests.post(\n",
    "        sbh_url,\n",
    "        headers={\"Accept\": \"text/plain\"},\n",
    "        data={\"email\": sbh_email, \"password\": sbh_password}\n",
    "    )\n",
    "\n",
    "    if response.ok:\n",
    "        token = response.text.strip() # theres a whitespace before the token for some reason\n",
    "        print(token)\n",
    "        exp_labels = [\"ExperimentalData_\" + label for label in exp_data_labels]  \n",
    "        \n",
    "        for label, file_name in zip(exp_labels, file_names):\n",
    "            attachment_uri = f\"https://synbiohub.org/user/{synbio_username}/{collection_name}/{label}/{version}/attach\"\n",
    "            print(attachment_uri)\n",
    "            with open(file_name, 'rb') as f:\n",
    "                response = requests.post(\n",
    "                    attachment_uri,\n",
    "                    headers={\n",
    "                        'X-authorization': token,\n",
    "                        'Accept': 'text/plain'\n",
    "                    },\n",
    "                    files={'file': f}\n",
    "                )\n",
    "    \n",
    "            \n",
    "            print(\"Status Code:\", response.status_code)\n",
    "            print(\"Response Body:\", response.text)\n",
    "    else:\n",
    "        print(\"Login failed:\", response.status_code)\n",
    "        print(response.text)\n",
    "\n",
    "def post_attachment(attachment_uri, file_path, email, password):\n",
    "    sbh_url = \"https://synbiohub.org/login\" \n",
    "    sbh_email = email\n",
    "    sbh_password = password\n",
    "    \n",
    "    response = requests.post(\n",
    "        sbh_url,\n",
    "        headers={\"Accept\": \"text/plain\"},\n",
    "        data={\"email\": sbh_email, \"password\": sbh_password}\n",
    "    )\n",
    "    \n",
    "    if response.ok:\n",
    "        token = response.text.strip() # theres a whitespace before the token for some reason\n",
    "        print(token)\n",
    "\n",
    "        with open(file_path, 'rb') as f:\n",
    "            response = requests.post(\n",
    "                attachment_uri,\n",
    "                headers={\n",
    "                    'X-authorization': token,\n",
    "                    'Accept': 'text/plain'\n",
    "                },\n",
    "                files={'file': f}\n",
    "            )\n",
    "        \n",
    "        print(\"Status Code:\", response.status_code)\n",
    "        print(\"Response Body:\", response.text)\n",
    "    \n",
    "    else:\n",
    "        print(\"Login failed:\", response.status_code)\n",
    "        print(response.text)\n",
    "\n",
    "#df1 and df2\n",
    "\n",
    "# fragment: 150–300 bp genomic DNA sequence that were randomly sheared and barcoded\n",
    "# RNA_exp_1: RNA expression level (replicate 1) – normalized measurement of transcript abundance for this fragment in the first RNA-Seq replicate. (normalized by DNA)\n",
    "# RNA_exp_2: RNA expression level (replicate 2) – same as above, but from a second biological replicate.\n",
    "# RNA_exp_ave: Average RNA expression – mean of RNA_exp_1 and RNA_exp_2\n",
    "# DNA_sum_1: DNA integration abundance (replicate 1) – quantifies how much of this DNA fragment (or barcode) was actually integrated in the first DNA-Seq sample. \n",
    "# DNA_sum_2: DNA integration abundance (replicate 2) – same as above, from the second replicate.\n",
    "# DNA_ave: Average DNA integration level – mean of DNA_sum_1 and DNA_sum_2\n",
    "# num_integrated_barcodes: Number of barcodes that were integrated into the genome for this fragment. Typically should match num_mapped_barcodes unless there are sequencing/integration issues.\n",
    "# start: Start coordinate (in bp) in the E. coli MG1655 reference genome (U00096.2). \n",
    "# end: End coordinate in the E. coli MG1655 reference genome (U00096.2). \n",
    "# strand: DNA strand orientation (+ or -) \n",
    "# variation: Standard deviation or variability in RNA measurement across barcode replicates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f37a6716-fc16-4896-aa19-c85b3902f471",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, random\n",
    "np.random.seed(1)\n",
    "random.seed(1)\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split  # sklearn >= 0.18\n",
    "import sys\n",
    "import argparse\n",
    "num_epochs = 100\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "\n",
    "# \tparser = argparse.ArgumentParser()\n",
    "# \tparser.add_argument('train', help='''pre-defined training set, \n",
    "# \t\tone column sequence, one column expression. Tab-separated''')\n",
    "# \tparser.add_argument('test', help='pre-defined test set')\n",
    "# \tparser.add_argument('seq_length', type=int, help='length of input sequences')\n",
    "# \tparser.add_argument('num_layers', type=int, help='number of convolutional layers')\n",
    "# \tparser.add_argument('min_filter', type=int, help='minimum number of filters')\n",
    "# \tparser.add_argument('max_filter', type=int, help='maximum number of filters')\n",
    "# \tparser.add_argument('validation_fraction', type=float)\n",
    "# \tparser.add_argument('num_trials', type=int, \n",
    "# \t\thelp='number of hyperparameter trials')\n",
    "# \tparser.add_argument('prefix', help='output prefix for saved model files')\n",
    "# \tparser.add_argument('--validation', help='Optional pre-defined validation set')\n",
    "\t\n",
    "# \targs = parser.parse_args()\n",
    "\n",
    "# \t# load in pre-defined splits\n",
    "# \tseq_length = args.seq_length\n",
    "# \tprint(\"loading training set...\")\n",
    "# \tX_train, y_train = process_seqs(args.train, seq_length)\n",
    "# \tprint(\"loading test set...\")\n",
    "# \tX_test, y_test = process_seqs(args.test, seq_length)\n",
    "\n",
    "\t\n",
    "# \tnum_layers = args.num_layers\n",
    "# \tmin_filter = args.min_filter\n",
    "# \tmax_filter = args.max_filter\n",
    "# \tvalidation_fraction = args.validation_fraction\n",
    "# \tnum_hyperparameter_trials = args.num_trials\n",
    "# \tprefix = args.prefix\n",
    "\n",
    "# \tif args.validation:\n",
    "# \t\tX_valid, y_valid = process_seqs(args.validation, seq_length)\n",
    "# \telse:\n",
    "# \t\t# split training into validation set\n",
    "# \t\tX_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, \n",
    "# \t\t\ttest_size=validation_fraction)\n",
    "\n",
    "\n",
    "# \tprint('Starting hyperparameter search...')\n",
    "# \tmin_layer = 1\n",
    "# \tmax_layer = 4\n",
    "# \tmin_conv_width = 15\n",
    "# \tmax_conv_width = 20\n",
    "# \tmin_dropout = 0.1\n",
    "# \tmax_dropout = 0.9\n",
    "\n",
    "# \tfixed_hyperparameters = {'seq_length': seq_length, 'num_epochs': num_epochs}\n",
    "# \tgrid = {'num_filters': ((min_filter, max_filter),), 'pool_width': (5, 40),\n",
    "# \t        'conv_width': ((min_conv_width, max_conv_width),), \n",
    "# \t        'dropout': (min_dropout, max_dropout)}\n",
    "\n",
    "# \t# number of convolutional layers        \n",
    "# \tprint(\"Number of convolutional layers: \", num_layers)\n",
    "# \tfilters = tuple([(min_filter, max_filter)] * num_layers)\n",
    "# \tconv_widths = tuple([(min_conv_width, max_conv_width)] * num_layers)\n",
    "# \tgrid.update({'num_filters': filters, 'conv_width': conv_widths})\n",
    "\n",
    "# \t# Backend is RandomSearch; if using Python 2, can also specify MOESearch\n",
    "# \t# (requires separate installation)\n",
    "# \tsearcher = HyperparameterSearcher(SequenceDNN, fixed_hyperparameters, grid, \n",
    "# \t\tX_train, y_train, validation_data=(X_valid, y_valid), backend=RandomSearch)\n",
    "# \tsearcher.search(num_hyperparameter_trials)\n",
    "# \tprint('Best hyperparameters: {}'.format(searcher.best_hyperparameters))\n",
    "# \tmodel = searcher.best_model\n",
    "\t\n",
    "# \t# print test results\n",
    "# \tprint('Test results: {}'.format(model.score(X_test, y_test)))\n",
    "\t\n",
    "# \t# save model\n",
    "# \tmodel.save(prefix)\n",
    "\t\n",
    "# \t# print predictions\n",
    "# \tpredictions = model.predict(X_test)\n",
    "# \ttest_sequences = [line.split('\\t')[0] for line in open(args.test)]\n",
    "\t\n",
    "# \twith open(prefix + '_predictions.txt', 'w') as outfile:\n",
    "# \t\tfor i in range(len(predictions)):\n",
    "# \t\t\toutfile.write(\n",
    "# \t\t\t\ttest_sequences[i] + '\\t' + \n",
    "# \t\t\t\tstr(float(predictions[i])) + '\\t' + \n",
    "# \t\t\t\tstr(float(y_test[i])) + '\\n')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e78b5634-1dfe-4f87-b7a6-a5de6546d1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode_modern(sequences):\n",
    "    sequences_array = np.array([[char for char in seq] for seq in sequences])\n",
    "    sequence_length = sequences_array.shape[1]\n",
    "    num_samples = sequences_array.shape[0]\n",
    "\n",
    "    defined_categories = ['A', 'C', 'G', 'T', 'N']\n",
    "    ohe = OneHotEncoder(sparse=False,\n",
    "                        categories=[defined_categories] * sequence_length,\n",
    "                        dtype=np.float32) # Still keep this as float32\n",
    "\n",
    "    one_hot_encoding_flat = ohe.fit_transform(sequences_array)\n",
    "\n",
    "    # Reshape to (num_samples, sequence_length, num_bases)\n",
    "    one_hot_encoding_reshaped = one_hot_encoding_flat.reshape(\n",
    "        num_samples, sequence_length, len(defined_categories))\n",
    "\n",
    "    # Add the 'height' dimension (which will be 1 for a 1D sequence interpreted as 2D)\n",
    "    # and ensure channels are last: (N, H, W, C) -> (num_samples, 1, sequence_length, 4)\n",
    "    # This becomes (num_samples, height=1, width=sequence_length, channels=4)\n",
    "    # Keras Conv2D with channels_last expects (batch, rows, cols, channels)\n",
    "    # For sequence, if you treat rows=1, cols=seq_length, channels=4\n",
    "    # then it should be (N, 1, seq_length, 4)\n",
    "    final_one_hot_encoding = one_hot_encoding_reshaped[:, np.newaxis, :, :] # (N, 1, L, 4)\n",
    "    # If your Conv2D is truly 2D over a sequence, this shape is fine.\n",
    "    # If it's more like 1D conv, you might need (N, L, C) and then a Reshape for Conv2D.\n",
    "    # Given your current Conv2D setup and input_shape:\n",
    "    # Conv2D(..., input_shape=(1, 4, seq_length), data_format='channels_first')\n",
    "    # You want Conv2D(..., input_shape=(1, seq_length, 4), data_format='channels_last')\n",
    "\n",
    "    # Let's target: (num_samples, height, width, channels)\n",
    "    # For your 4-channel DNA, it's (num_samples, 1, seq_length, 4) or just (num_samples, seq_length, 4) if it's a \"1D\" Conv2D.\n",
    "    # Given your `conv_height=4` for the first layer, it seems you want the 4 bases as height.\n",
    "    # This implies: (N, 4, seq_length, 1) or (N, seq_length, 4, 1) as input, if you want to use channels_last.\n",
    "    # The original code's input_shape=(1, 4, seq_length) and conv_height=4 suggests NCHW (N, C, H, W) where C=1.\n",
    "    # If C=1, then it's (N, 1, 4, seq_length) where 4 is the 'height' and seq_length is 'width'.\n",
    "    # For channels_last, that would be (N, 4, seq_length, 1).\n",
    "\n",
    "    # So, from (num_samples, sequence_length, len(defined_categories))\n",
    "    # you want (num_samples, Height=4, Width=sequence_length, Channels=1)\n",
    "\n",
    "    # Transpose to get channels last if your original was (N, C, H, W)\n",
    "    # From (num_samples, sequence_length, 4) where 4 are the one-hot channels\n",
    "    # We need (num_samples, H, W, C) where H=4, W=seq_length, C=1\n",
    "    # This is a bit tricky given your initial Conv2D setup.\n",
    "\n",
    "    # Let's simplify the one-hot encoding output:\n",
    "    # Output directly (num_samples, seq_length, 4) as (batch, steps, features)\n",
    "    # Then we'll reshape for Conv2D inside SequenceDNN if needed.\n",
    "    # (N, L, 4) where L is sequence_length, 4 is features (A,C,G,T).\n",
    "    return one_hot_encoding_reshaped # (N, L, 4)\n",
    "    return final_one_hot_encoding\n",
    "\n",
    "def pad_sequence(seq, max_length):\n",
    "\tif len(seq) > max_length:\n",
    "\t\tdiff = len(seq) - max_length\n",
    "\t\ttrim_length = int(diff / 2)\n",
    "\t\tseq = seq[trim_length : -(trim_length + diff%2)]\n",
    "\telse:\n",
    "\t\tseq = seq.center(max_length, 'N')\n",
    "\treturn seq\n",
    "\n",
    "def process_seqs(df, seq_length):\n",
    "\tpadded_seqs = [pad_sequence(x, seq_length) for x in df['variant']]\n",
    "\tX = one_hot_encode_modern(np.array(padded_seqs))\n",
    "\ty = np.array(df['expn_med_fitted_scaled'], dtype=np.float32)\n",
    "\treturn X, y\n",
    "\n",
    "\n",
    "def split_data_by_peak(data_df):\n",
    "    np.random.seed(123)\n",
    "\n",
    "    # 1. Create 'peak_name' column (if not already present)\n",
    "    # Assuming peak_start and peak_end are numeric, convert to string for concatenation\n",
    "    data_df['peak_name'] = data_df['peak_start'].astype(str) + 'to' + data_df['peak_end'].astype(str)\n",
    "\n",
    "    # 2. Get unique peak names\n",
    "    peak_names = data_df['peak_name'].unique()\n",
    "\n",
    "    # 3. Randomly sample train peak names (90%)\n",
    "    train_size = int(0.90 * len(peak_names))\n",
    "    train_peak_names = np.random.choice(peak_names, size=train_size, replace=False)\n",
    "\n",
    "    # 4. Determine test peak names\n",
    "    test_peak_names = np.array([p for p in peak_names if p not in train_peak_names])\n",
    "\n",
    "\n",
    "    data_train = data_df[data_df['peak_name'].isin(train_peak_names)].copy()\n",
    "    data_test = data_df[data_df['peak_name'].isin(test_peak_names)].copy()\n",
    "\n",
    "    return data_train, data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "734071ac-e41c-4789-92e6-37a8bc4fb65f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function BaseSession._Callable.__del__ at 0x000001B11311CAE8>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Sai\\anaconda3\\envs\\tf1.0\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1455, in __del__\n",
      "    self._session._session, self._handle, status)\n",
      "  File \"C:\\Users\\Sai\\anaconda3\\envs\\tf1.0\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\", line 528, in __exit__\n",
      "    c_api.TF_GetCode(self.status.status))\n",
      "tensorflow.python.framework.errors_impl.InvalidArgumentError: No such callable handle: 0\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b1d5c26d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0., 1., 0.],\n",
       "        [0., 1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 1., 0.],\n",
       "        [0., 1., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 1., 0.],\n",
       "        [0., 1., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 1., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1.]],\n",
       "\n",
       "       [[0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 1.],\n",
       "        ...,\n",
       "        [0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 1.]],\n",
       "\n",
       "       [[0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 1.],\n",
       "        ...,\n",
       "        [0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 1.]]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "08d67082-2e3a-44f7-a75f-2d6651ff65cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "import numpy as np\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "import tempfile\n",
    "from abc import abstractmethod, ABCMeta\n",
    "\n",
    "class Model(object):\n",
    "    __metaclass__ = ABCMeta\n",
    "\n",
    "    @abstractmethod\n",
    "    def __init__(self, **hyperparameters):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def train(self, X, y, validation_data):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(self, X):\n",
    "        pass\n",
    "\n",
    "    def test(self, X, y):\n",
    "        return self.evaluate(X, y)\n",
    "        # return ClassificationResult(y, self.predict(X))\n",
    "\n",
    "    def score(self, X, y):\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "class SequenceDNN(Model):\n",
    "    \"\"\"\n",
    "    Sequence DNN models, regression. No activation layer\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    seq_length : int, optional\n",
    "        length of input sequence.\n",
    "    keras_model : instance of keras.models.Sequential, optional\n",
    "        seq_length or keras_model must be specified.\n",
    "    num_tasks : int, optional\n",
    "        number of tasks. Default: 1.\n",
    "    num_filters : list[int] | tuple[int]\n",
    "        number of convolutional filters in each layer. Default: (15,).\n",
    "    conv_width : list[int] | tuple[int]\n",
    "        width of each layer's convolutional filters. Default: (15,).\n",
    "    pool_width : int\n",
    "        width of max pooling after the last layer. Default: 35.\n",
    "    L1 : float\n",
    "        strength of L1 penalty.\n",
    "    dropout : float\n",
    "        dropout probability in every convolutional layer. Default: 0.\n",
    "    verbose: int\n",
    "        Verbosity level during training. Valida values: 0, 1, 2.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Compiled DNN model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, seq_length=None, keras_model=None,\n",
    "                 use_RNN=False, num_tasks=1,\n",
    "                 num_filters=(15, 15, 15), conv_width=(15, 15, 15),\n",
    "                 pool_width=35, GRU_size=35, TDD_size=15,\n",
    "                 L1=0, dropout=0.0, num_epochs=100, verbose=1):\n",
    "        from tensorflow.keras.models import Sequential\n",
    "        from tensorflow.keras.layers import (\n",
    "            Activation, Dense, Dropout, Flatten,\n",
    "            Permute, Reshape)\n",
    "        from tensorflow.keras.layers import TimeDistributed\n",
    "        from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "        from tensorflow.keras.layers import GRU\n",
    "        from tensorflow.keras.regularizers import l1\n",
    "        self.num_tasks = num_tasks\n",
    "        self.num_epochs = num_epochs\n",
    "        self.verbose = verbose\n",
    "        self.train_metrics = []\n",
    "        self.valid_metrics = []\n",
    "        if keras_model is not None and seq_length is None:\n",
    "            self.model = keras_model\n",
    "            self.name = 'Sequential'\n",
    "            self.num_tasks = keras_model.layers[-1].output_shape[-1]\n",
    "        elif seq_length is not None and keras_model is None:\n",
    "            self.model = Sequential()\n",
    "            assert len(num_filters) == len(conv_width)\n",
    "            for i, (nb_filter, nb_col) in enumerate(zip(num_filters, conv_width)):\n",
    "                conv_height = 4 if i == 0 else 1\n",
    "                # self.model.add(Convolution2D(\n",
    "                #     nb_filter=nb_filter, nb_row=conv_height,\n",
    "                #     nb_col=nb_col, activation='linear',\n",
    "                #     init='he_normal', input_shape=(1, 4, seq_length),\n",
    "                #     W_regularizer=l1(L1), b_regularizer=l1(L1)))\n",
    "                \n",
    "                self.model.add(Conv2D(filters=nb_filter, \n",
    "                        kernel_size=(conv_height, nb_col),\n",
    "                        activation='linear', kernel_initializer='he_normal',\n",
    "                        input_shape=(1, 4, seq_length),\n",
    "                        kernel_regularizer=l1(L1), bias_regularizer=l1(L1),\n",
    "                        data_format='channels_last'))\n",
    "                self.model.add(Activation('relu'))\n",
    "                self.model.add(Dropout(dropout))\n",
    "            self.model.add(MaxPooling2D(pool_size=(1, pool_width),\n",
    "                data_format='channels_first'))\n",
    "            if use_RNN:\n",
    "                num_max_pool_outputs = self.model.layers[-1].output_shape[-1]\n",
    "                self.model.add(Reshape((num_filters[-1], num_max_pool_outputs)))\n",
    "                self.model.add(Permute((2, 1)))\n",
    "                self.model.add(GRU(GRU_size, return_sequences=True))\n",
    "                self.model.add(TimeDistributed(TDD_size, activation='relu'))\n",
    "            self.model.add(Flatten())\n",
    "            self.model.add(Dense(units=self.num_tasks))\n",
    "            # no activation layer, MSE loss\n",
    "            self.model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "        else:\n",
    "            raise ValueError(\"Exactly one of seq_length or keras_model must be specified!\")\n",
    "\n",
    "    def train(self, X, y, validation_data, early_stopping_metric='Loss',\n",
    "              early_stopping_patience=5, save_best_model_to_prefix=None):\n",
    "\n",
    "        if self.verbose >= 1:\n",
    "            print('Training model (* indicates new best result)...')\n",
    "        X_valid, y_valid = validation_data\n",
    "        early_stopping_wait = 0\n",
    "        best_metric = np.inf if early_stopping_metric == 'Loss' else -np.inf\n",
    "        # self.model.fit(X, y, epochs=self.num_epochs+1, verbose=self.verbose >= 2)\n",
    "        # score = self.test(X_valid, y_valid)\n",
    "        # print('Test loss:', score[0])\n",
    "        # print('Test accuracy:', score[1])\n",
    "        for epoch in range(1, self.num_epochs + 1):\n",
    "            self.model.fit(X, y, batch_size=128, epochs=1, verbose=0)\n",
    "            epoch_train_metrics = self.model.evaluate(X, y, verbose=0)\n",
    "            epoch_valid_metrics = self.model.evaluate(X_valid, y_valid, verbose=0)\n",
    "            self.train_metrics.append(epoch_train_metrics)\n",
    "            self.valid_metrics.append(epoch_valid_metrics)\n",
    "            if self.verbose >= 1:\n",
    "                print('Epoch {}:'.format(epoch))\n",
    "                print('Train: ', epoch_train_metrics)\n",
    "                print('Valid: ', epoch_valid_metrics)\n",
    "            current_metric = epoch_valid_metrics\n",
    "            if current_metric <= best_metric:\n",
    "                if self.verbose >= 1:\n",
    "                    print(' *')\n",
    "                best_metric = current_metric\n",
    "                best_epoch = epoch\n",
    "                early_stopping_wait = 0\n",
    "                if save_best_model_to_prefix is not None:\n",
    "                    self.save(save_best_model_to_prefix)\n",
    "            else:\n",
    "                if early_stopping_wait >= early_stopping_patience:\n",
    "                    break\n",
    "                early_stopping_wait += 1\n",
    "\n",
    "        if self.verbose >= 1:\n",
    "            print('Finished training after {} epochs.'.format(epoch))\n",
    "            if save_best_model_to_prefix is not None:\n",
    "                print(\"The best model's architecture and weights (from epoch {0}) \"\n",
    "                      'were saved to {1}.arch.json and {1}.weights.h5'.format(\n",
    "                    best_epoch, save_best_model_to_prefix))\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)\n",
    "\n",
    "\n",
    "    def score(self, X, y):\n",
    "        predictions = np.squeeze(self.model.predict(X))\n",
    "        return np.corrcoef(predictions, y)[0,1]\n",
    "\n",
    "    def save(self, save_best_model_to_prefix):\n",
    "        arch_fname = save_best_model_to_prefix + '.arch.json'\n",
    "        weights_fname = save_best_model_to_prefix + '.weights.h5'\n",
    "        open(arch_fname, 'w').write(self.model.to_json())\n",
    "        self.model.save_weights(weights_fname, overwrite=True)\n",
    "\n",
    "    @staticmethod\n",
    "    def load(arch_fname, weights_fname=None):\n",
    "        from keras.models import model_from_json\n",
    "        model_json_string = open(arch_fname).read()\n",
    "        sequence_dnn = SequenceDNN(keras_model=model_from_json(model_json_string))\n",
    "        if weights_fname is not None:\n",
    "            sequence_dnn.model.load_weights(weights_fname)\n",
    "        return sequence_dnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "58348a36-83c0-49ab-89b1-1b816180c515",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "import numpy as np, sys\n",
    "from abc import abstractmethod, ABCMeta\n",
    "\n",
    "\n",
    "class HyperparameterBackend(object):\n",
    "    __metaclass__ = ABCMeta\n",
    "\n",
    "    @abstractmethod\n",
    "    def __init__(self, grid):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        grid: dict\n",
    "            Keys are hyperparameter names and values are either\n",
    "            a single (min, max) tuple for single value parameters\n",
    "            or a tuple of (min, max) tuples for tuple-valued parameters.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_next_hyperparameters(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def record_result(self, hyperparam_dict, score):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        hyperparam_dict: dict\n",
    "            hyperparameter names as keys and values as values.\n",
    "        score: int or float\n",
    "            The result, or metric value, of using the hyparameters.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class RandomSearch(HyperparameterBackend):\n",
    "    def __init__(self, grid):\n",
    "        self.grid = grid\n",
    "\n",
    "    def get_next_hyperparameters(self):\n",
    "        return [np.random.uniform(start, end) for start, end in self.grid]\n",
    "\n",
    "    def record_result(self, hyperparam_dict, score):\n",
    "        pass  # Random search doesn't base its decisions on the results of previous trials\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class HyperparameterSearcher(object):\n",
    "    def __init__(self, model_class, fixed_hyperparameters, grid, X_train, y_train, validation_data,\n",
    "                 maximize=True, backend=RandomSearch):\n",
    "        self.model_class = model_class\n",
    "        self.fixed_hyperparameters = fixed_hyperparameters\n",
    "        self.grid = grid\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.validation_data = validation_data\n",
    "        self.maximize = maximize\n",
    "        self.best_score = 0\n",
    "        self.best_model = self.best_hyperparameters = None\n",
    "        # Some hyperparameters have multiple elements, and we need backend to treat each of them\n",
    "        # as a separate dimension, so unpack them here.\n",
    "        backend_grid = [bounds for value in grid.values()\n",
    "                        for bounds in (value if isinstance(value[0], (list, tuple, np.ndarray))\n",
    "                                       else (value,))]\n",
    "        self.backend = backend(backend_grid)\n",
    "\n",
    "    def search(self, num_hyperparameter_trials):\n",
    "        for trial in range(num_hyperparameter_trials):\n",
    "     \n",
    "            print(\"Trial \", trial)\n",
    "            # Select next hyperparameters with MOE, rounding hyperparameters that are integers\n",
    "            # and re-packing multi-element hyperparameters\n",
    "            raw_hyperparameters = self.backend.get_next_hyperparameters()\n",
    "            hyperparameters = {}\n",
    "            i = 0\n",
    "            for name, bounds in self.grid.items():\n",
    "                if isinstance(bounds[0], (list, tuple, np.ndarray)):\n",
    "                    # Multi-element hyperparameter\n",
    "                    hyperparameters[name] = raw_hyperparameters[i : i + len(bounds)]\n",
    "                    if isinstance(bounds[0][0], int):\n",
    "                        hyperparameters[name] = np.rint(hyperparameters[name]).astype(int)\n",
    "                    i += len(bounds)\n",
    "                else:\n",
    "                    hyperparameters[name] = raw_hyperparameters[i]\n",
    "                    if isinstance(bounds[0], int):\n",
    "                        hyperparameters[name] = int(round(hyperparameters[name]))\n",
    "                    i += 1\n",
    "            assert i == len(raw_hyperparameters)\n",
    "            print(hyperparameters)\n",
    "            # Try these hyperparameters\n",
    "            model = self.model_class(**{key: value\n",
    "                                        for dictionary in (hyperparameters, self.fixed_hyperparameters)\n",
    "                                        for key, value in dictionary.items()})\n",
    "            print(y_train.dtype)\n",
    "            model.train(self.X_train, self.y_train, validation_data=self.validation_data)\n",
    "            \n",
    "            # task_scores = model.score(self.validation_data[0], self.validation_data[1], self.metric)\n",
    "            # score = task_scores.mean()  # mean across tasks\n",
    "            # score = np.corrcoef(np.squeeze(model.predict(X_valid)), y_valid)[0,1]\n",
    "            score = model.score(self.validation_data[0], self.validation_data[1])\n",
    "            print(\"Valid: correlation between predicted and observed:\", score)\n",
    "\n",
    "            # Record hyperparameters and validation loss\n",
    "            self.backend.record_result(hyperparameters, score)\n",
    "            # If these hyperparameters were the best so far, store this model\n",
    "            if self.maximize == (score > self.best_score):\n",
    "                self.best_score = score\n",
    "                self.best_model = model\n",
    "                self.best_hyperparameters = hyperparameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cdeab80b-7291-421f-ba1d-eb0009bd3280",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_CNN(X_train, y_train, X_test, y_test, seq_length, num_layers, min_filter, max_filter, validation_fraction, num_hyperparameter_trials, prefix):\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=validation_fraction)\n",
    "    print('Starting hyperparameter searchererereer...')\n",
    "    print(X_test.dtype)\n",
    "    min_layer = 1\n",
    "    max_layer = 4\n",
    "    min_conv_width = 15\n",
    "    max_conv_width = 20\n",
    "    min_dropout = 0.1\n",
    "    max_dropout = 0.9\n",
    "    \n",
    "    fixed_hyperparameters = {'seq_length': seq_length, 'num_epochs': num_epochs}\n",
    "    grid = {'num_filters': ((min_filter, max_filter),), 'pool_width': (5, 40),\n",
    "            'conv_width': ((min_conv_width, max_conv_width),), \n",
    "            'dropout': (min_dropout, max_dropout)}\n",
    "    \n",
    "    # number of convolutional layers        \n",
    "    print(\"Number of convolutional layers: \", num_layers)\n",
    "    filters = tuple([(min_filter, max_filter)] * num_layers)\n",
    "    conv_widths = tuple([(min_conv_width, max_conv_width)] * num_layers)\n",
    "    grid.update({'num_filters': filters, 'conv_width': conv_widths})\n",
    "    \n",
    "    # Backend is RandomSearch; if using Python 2, can also specify MOESearch\n",
    "    # (requires separate installation)\n",
    "    searcher = HyperparameterSearcher(SequenceDNN, fixed_hyperparameters, grid, \n",
    "        X_train, y_train, validation_data=(X_valid, y_valid), backend=RandomSearch)\n",
    "    searcher.search(num_hyperparameter_trials)\n",
    "    print('Best hyperparameters: {}'.format(searcher.best_hyperparameters))\n",
    "    model = searcher.best_model\n",
    "    \n",
    "    # print test results\n",
    "    print('Test results: {}'.format(model.score(X_test, y_test)))\n",
    "    \n",
    "    # save model\n",
    "    model.save(prefix)\n",
    "    \n",
    "    # print predictions\n",
    "    predictions = model.predict(X_test)\n",
    "    test_sequences = [line.split('\\t')[0] for line in open(args.test)]\n",
    "    \n",
    "    with open(prefix + '_predictions.txt', 'w') as outfile:\n",
    "        for i in range(len(predictions)):\n",
    "            outfile.write(\n",
    "                test_sequences[i] + '\\t' + \n",
    "                str(float(predictions[i])) + '\\t' + \n",
    "                str(float(y_test[i])) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "33e9e992-9de3-472f-b59f-b0ecddd7b25f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting hyperparameter searchererereer...\n",
      "float32\n",
      "Number of convolutional layers:  4\n",
      "Trial  0\n",
      "{'num_filters': array([94, 77, 77, 60]), 'pool_width': 14, 'conv_width': array([17, 19, 16, 19]), 'dropout': 0.1883397207778515}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Negative dimension size caused by subtracting 4 from 1 for 'conv2d_4/Conv2D' (op: 'Conv2D') with input shapes: [?,1,4,150], [4,17,150,94].",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\envs\\tf1.0\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[1;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[0;32m   1658\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1659\u001b[1;33m     \u001b[0mc_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_FinishOperation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1660\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Negative dimension size caused by subtracting 4 from 1 for 'conv2d_4/Conv2D' (op: 'Conv2D') with input shapes: [?,1,4,150], [4,17,150,94].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_500\\3690144753.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_CNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m150\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_filter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_filter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_fraction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_hyperparameter_trials\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"results\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_500\\3930654137.py\u001b[0m in \u001b[0;36mtrain_CNN\u001b[1;34m(X_train, y_train, X_test, y_test, seq_length, num_layers, min_filter, max_filter, validation_fraction, num_hyperparameter_trials, prefix)\u001b[0m\n\u001b[0;32m     25\u001b[0m     searcher = HyperparameterSearcher(SequenceDNN, fixed_hyperparameters, grid, \n\u001b[0;32m     26\u001b[0m         X_train, y_train, validation_data=(X_valid, y_valid), backend=RandomSearch)\n\u001b[1;32m---> 27\u001b[1;33m     \u001b[0msearcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_hyperparameter_trials\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Best hyperparameters: {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msearcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_hyperparameters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msearcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_500\\4221802389.py\u001b[0m in \u001b[0;36msearch\u001b[1;34m(self, num_hyperparameter_trials)\u001b[0m\n\u001b[0;32m     93\u001b[0m             \u001b[1;31m# Try these hyperparameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m             model = self.model_class(**{key: value\n\u001b[1;32m---> 95\u001b[1;33m                                         \u001b[1;32mfor\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhyperparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfixed_hyperparameters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     96\u001b[0m                                         for key, value in dictionary.items()})\n\u001b[0;32m     97\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_500\\3366211932.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, seq_length, keras_model, use_RNN, num_tasks, num_filters, conv_width, pool_width, GRU_size, TDD_size, L1, dropout, num_epochs, verbose)\u001b[0m\n\u001b[0;32m     99\u001b[0m                         \u001b[0minput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m                         \u001b[0mkernel_regularizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0ml1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mL1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias_regularizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0ml1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mL1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 101\u001b[1;33m                         data_format='channels_last'))\n\u001b[0m\u001b[0;32m    102\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mActivation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf1.0\\lib\\site-packages\\tensorflow\\python\\training\\checkpointable\\base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    440\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_setattr_tracking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    441\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 442\u001b[1;33m       \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    443\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    444\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_setattr_tracking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf1.0\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py\u001b[0m in \u001b[0;36madd\u001b[1;34m(self, layer)\u001b[0m\n\u001b[0;32m    162\u001b[0m           \u001b[1;31m# and create the node connecting the current layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m           \u001b[1;31m# to the input layer we just created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 164\u001b[1;33m           \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    165\u001b[0m           \u001b[0mset_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf1.0\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[1;31m# In graph mode, failure to build the layer's graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    553\u001b[0m             \u001b[1;31m# implies a user-side bug. We don't catch exceptions.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 554\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    555\u001b[0m           \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    556\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf1.0\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\convolutional.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 194\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_convolution_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf1.0\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inp, filter)\u001b[0m\n\u001b[0;32m    964\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    965\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=redefined-builtin\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 966\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    967\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    968\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf1.0\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inp, filter)\u001b[0m\n\u001b[0;32m    589\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=redefined-builtin\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 591\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    592\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf1.0\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inp, filter)\u001b[0m\n\u001b[0;32m    206\u001b[0m         \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[0mdata_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         name=self.name)\n\u001b[0m\u001b[0;32m    209\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf1.0\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d\u001b[1;34m(input, filter, strides, padding, use_cudnn_on_gpu, data_format, dilations, name)\u001b[0m\n\u001b[0;32m   1110\u001b[0m         \u001b[1;34m\"Conv2D\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstrides\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1111\u001b[0m                   \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1112\u001b[1;33m                   data_format=data_format, dilations=dilations, name=name)\n\u001b[0m\u001b[0;32m   1113\u001b[0m   \u001b[0m_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1114\u001b[0m   \u001b[0m_inputs_flat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf1.0\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[1;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    786\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[0;32m    787\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 788\u001b[1;33m                          op_def=op_def)\n\u001b[0m\u001b[0;32m    789\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_stateful\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    790\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf1.0\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    505\u001b[0m                 \u001b[1;34m'in a future version'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'after %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    506\u001b[0m                 instructions)\n\u001b[1;32m--> 507\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    508\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    509\u001b[0m     doc = _add_deprecated_arg_notice_to_docstring(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf1.0\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   3298\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3299\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3300\u001b[1;33m           op_def=op_def)\n\u001b[0m\u001b[0;32m   3301\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_op_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompute_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompute_device\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3302\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf1.0\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[0;32m   1821\u001b[0m           op_def, inputs, node_def.attr)\n\u001b[0;32m   1822\u001b[0m       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,\n\u001b[1;32m-> 1823\u001b[1;33m                                 control_input_ops)\n\u001b[0m\u001b[0;32m   1824\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1825\u001b[0m     \u001b[1;31m# Initialize self._outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf1.0\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[1;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[0;32m   1660\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1661\u001b[0m     \u001b[1;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1662\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1663\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1664\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mc_op\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Negative dimension size caused by subtracting 4 from 1 for 'conv2d_4/Conv2D' (op: 'Conv2D') with input shapes: [?,1,4,150], [4,17,150,94]."
     ]
    }
   ],
   "source": [
    "train_CNN(X_train, y_train, X_test, y_test, seq_length=150, num_layers=4, min_filter=5, max_filter=100, validation_fraction=0.2, num_hyperparameter_trials=100, prefix=\"results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f4ffd4aa-c904-47ba-b18e-6f94fcdd2840",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "import numpy as np\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "import tempfile\n",
    "# matplotlib.use('pdf')\n",
    "# import matplotlib.pyplot as plt\n",
    "from abc import abstractmethod, ABCMeta\n",
    "from sklearn.tree import DecisionTreeClassifier as scikit_DecisionTree\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "class Model(object):\n",
    "    __metaclass__ = ABCMeta\n",
    "\n",
    "    @abstractmethod\n",
    "    def __init__(self, **hyperparameters):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def train(self, X, y, validation_data):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(self, X):\n",
    "        pass\n",
    "\n",
    "    def test(self, X, y):\n",
    "        return self.evaluate(X, y)\n",
    "        # return ClassificationResult(y, self.predict(X))\n",
    "\n",
    "    def score(self, X, y):\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "class DecisionTree(Model):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.classifier = scikit_DecisionTree()\n",
    "\n",
    "    def train(self, X, y, validation_data=None):\n",
    "        self.classifier.fit(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = np.asarray(self.classifier.predict_proba(X))[..., 1]\n",
    "        if len(predictions.shape) == 2:  # multitask\n",
    "            predictions = predictions.T\n",
    "        else:  # single-task\n",
    "            predictions = np.expand_dims(predictions, 1)\n",
    "        return predictions\n",
    "class RandomForestRegression(DecisionTree):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.regressor = RandomForestRegressor(n_estimators=100)\n",
    "\n",
    "    def train(self, X, y, validation_data=None):\n",
    "        # X shape: n_samples, n_features\n",
    "        # y shape: n_samples\n",
    "        self.regressor.fit(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.regressor.predict(X)\n",
    "\n",
    "    def score(self, X, y):\n",
    "        predictions = np.squeeze(self.regressor.predict(X))\n",
    "        return np.corrcoef(predictions, y)[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d6c2477e-0820-44b3-9bdb-5a5240e9fd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode_modern(sequences):\n",
    "    sequences_array = np.array([[char for char in seq] for seq in sequences])\n",
    "    sequence_length = sequences_array.shape[1]\n",
    "    num_samples = sequences_array.shape[0]\n",
    "\n",
    "    defined_categories = ['A', 'C', 'G', 'T', 'N']\n",
    "    ohe = OneHotEncoder(sparse_output=False,\n",
    "                        categories=[defined_categories] * sequence_length,\n",
    "                        dtype=np.float32) # Still keep this as float32\n",
    "\n",
    "    return ohe.fit_transform(sequences_array)\n",
    "\n",
    "def pad_sequence(seq, max_length):\n",
    "\tif len(seq) > max_length:\n",
    "\t\tdiff = len(seq) - max_length\n",
    "\t\ttrim_length = int(diff / 2)\n",
    "\t\tseq = seq[trim_length : -(trim_length + diff%2)]\n",
    "\telse:\n",
    "\t\tseq = seq.center(max_length, 'N')\n",
    "\treturn seq\n",
    "\n",
    "def process_seqs(df, seq_length):\n",
    "\tpadded_seqs = [pad_sequence(x, seq_length) for x in df['variant']]\n",
    "\tX = one_hot_encode_modern(np.array(padded_seqs))\n",
    "\ty = np.array(df['expn_med_fitted_scaled'], dtype=np.float32)\n",
    "\treturn X, y\n",
    "\n",
    "\n",
    "def split_data_by_peak(data_df):\n",
    "    np.random.seed(123)\n",
    "\n",
    "    # 1. Create 'peak_name' column (if not already present)\n",
    "    # Assuming peak_start and peak_end are numeric, convert to string for concatenation\n",
    "    data_df['peak_name'] = data_df['peak_start'].astype(str) + 'to' + data_df['peak_end'].astype(str)\n",
    "\n",
    "    # 2. Get unique peak names\n",
    "    peak_names = data_df['peak_name'].unique()\n",
    "\n",
    "    # 3. Randomly sample train peak names (90%)\n",
    "    train_size = int(0.90 * len(peak_names))\n",
    "    train_peak_names = np.random.choice(peak_names, size=train_size, replace=False)\n",
    "\n",
    "    # 4. Determine test peak names\n",
    "    test_peak_names = np.array([p for p in peak_names if p not in train_peak_names])\n",
    "\n",
    "\n",
    "    data_train = data_df[data_df['peak_name'].isin(train_peak_names)].copy()\n",
    "    data_test = data_df[data_df['peak_name'].isin(test_peak_names)].copy()\n",
    "\n",
    "    return data_train, data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8372b0b1-6e21-44f8-9e00-45c3693325d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_df = df5\n",
    "train_df, test_df = split_data_by_peak(data_df.copy())\n",
    "\n",
    "X_train, y_train = process_seqs(train_df, 150)\n",
    "X_test, y_test = process_seqs(test_df, 150)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "88e0edab-aa2b-43a1-9fe6-b77411c4ed45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np, random\n",
    "np.random.seed(1)\n",
    "random.seed(1)\n",
    "from sklearn.model_selection import train_test_split  # sklearn >= 0.18\n",
    "import sys\n",
    "import argparse\n",
    "import time\n",
    "\n",
    "def train_RandomForest(X_train, X_test, y_train, y_test):\n",
    "    print(\"Running random forest regression...\")\n",
    "    model = RandomForestRegression()\n",
    "    model.train(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    \n",
    "    with open(\"outputs.txt\", 'w') as outfile:\n",
    "        for i in range(len(predictions)):\n",
    "            outfile.write(str(float(predictions[i])) + '\\t' +\n",
    "                      str(float(y_test[i])) + '\\n')\n",
    "    \n",
    "    score = model.score(X_test, y_test)\n",
    "    print(\"Score:\", score)\n",
    "    return model, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e104f186-43ed-4b50-9c54-e8377addc03a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running random forest regression...\n",
      "Score: 0.07955371559314629\n"
     ]
    }
   ],
   "source": [
    "model, predictions = train_RandomForest(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "482648da-d0b4-4116-840c-5ae4790d6f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 5.548276973597522\n",
      "Root Mean Square Error (RMSE): 2.3554780775030624\n",
      "R^2: -0.10019475191929983\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "r_squared = r2_score(y_test, predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"Root Mean Square Error (RMSE): {rmse}\")\n",
    "print(f\"R^2: {r_squared}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc449ca-b848-4dbb-8202-efb06b1ba463",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
