{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6169c8c4-6c46-41c1-9c84-3bff9d2e2c8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sbol2\n",
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "import zipfile\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "current_dir = os.path.abspath('')\n",
    "data_path = os.path.join(current_dir, '..', 'data')\n",
    "attachments_path = os.path.join(current_dir, '..', 'attachments')\n",
    "pulled_attachments_path = os.path.join(current_dir, '..', 'pulled_attachments')\n",
    "sbol_path = os.path.join(current_dir, '..', 'sbol_data')\n",
    "downloaded_sbol_path = os.path.join(current_dir, '..', 'downloaded_sbol')\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "05b881c7-b711-41ab-af99-5c3e0dcb7245",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(data_path, \"frag-rLP5_LB_expression.txt\"), delimiter=\" \")\n",
    "# Expression levels from genomic fragment MPRA (random 200–300 bp sheared fragments) in LB media\t\n",
    "df2 = pd.read_csv(os.path.join(data_path, \"frag-rLP5-M9_expression.txt\"), delimiter=\" \")\n",
    "# Expression levels from genomic fragment MPRA (random 200–300 bp sheared fragments) in M9 media at rLP5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "3d2d2488-94ef-4002-a271-e0e8813de09a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fragment</th>\n",
       "      <th>RNA_exp_1</th>\n",
       "      <th>RNA_exp_2</th>\n",
       "      <th>RNA_exp_ave</th>\n",
       "      <th>DNA_sum_1</th>\n",
       "      <th>DNA_sum_2</th>\n",
       "      <th>DNA_ave</th>\n",
       "      <th>num_mapped_barcodes</th>\n",
       "      <th>num_integrated_barcodes</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>strand</th>\n",
       "      <th>variation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CTTCCAACCCATGGCGCGTGCGTACATAAAGGTTTCGGACGCGCGC...</td>\n",
       "      <td>1.316755</td>\n",
       "      <td>1.635498</td>\n",
       "      <td>1.476127</td>\n",
       "      <td>1.006226</td>\n",
       "      <td>1.006226</td>\n",
       "      <td>1.006226</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1838405</td>\n",
       "      <td>1838702</td>\n",
       "      <td>-</td>\n",
       "      <td>0.312743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TGACTTCTTCGTGAACTTGCTGGATATGCGTTACGAGTGGAAAGCG...</td>\n",
       "      <td>0.776481</td>\n",
       "      <td>0.881533</td>\n",
       "      <td>0.829007</td>\n",
       "      <td>2.227730</td>\n",
       "      <td>2.227730</td>\n",
       "      <td>2.227730</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4133776</td>\n",
       "      <td>4134009</td>\n",
       "      <td>+</td>\n",
       "      <td>0.183064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TGGGGGGATGTATGGGTACGTTGTAATTAGGGATTTAACGAATTAG...</td>\n",
       "      <td>0.530471</td>\n",
       "      <td>1.426090</td>\n",
       "      <td>0.978280</td>\n",
       "      <td>2.721586</td>\n",
       "      <td>2.721586</td>\n",
       "      <td>2.721586</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2862818</td>\n",
       "      <td>2863076</td>\n",
       "      <td>-</td>\n",
       "      <td>1.426718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAAAATAAAATTCAGCAATCAATTAATGCCTTACATCAACATGGCA...</td>\n",
       "      <td>2.392780</td>\n",
       "      <td>2.118698</td>\n",
       "      <td>2.255739</td>\n",
       "      <td>5.572886</td>\n",
       "      <td>5.572886</td>\n",
       "      <td>5.572886</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3798297</td>\n",
       "      <td>3798568</td>\n",
       "      <td>-</td>\n",
       "      <td>0.175510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TTTTTTGTCGTTGACCTCACCATGTCGATCACTGTGCCTGTATCCC...</td>\n",
       "      <td>0.664520</td>\n",
       "      <td>0.619367</td>\n",
       "      <td>0.641943</td>\n",
       "      <td>1.163899</td>\n",
       "      <td>1.163899</td>\n",
       "      <td>1.163899</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3657099</td>\n",
       "      <td>3657389</td>\n",
       "      <td>+</td>\n",
       "      <td>0.101518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AAAGACAAAGCACTTCTTCAAGAGTTGTGCTTTGGCGTACTGCGTA...</td>\n",
       "      <td>0.686980</td>\n",
       "      <td>0.558180</td>\n",
       "      <td>0.622580</td>\n",
       "      <td>6.865710</td>\n",
       "      <td>6.865710</td>\n",
       "      <td>6.865710</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3433342</td>\n",
       "      <td>3433583</td>\n",
       "      <td>+</td>\n",
       "      <td>0.299536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CTATTTGCTCGCAGGTCTGTTTGGTAGTGCCATTACCGGGATGATG...</td>\n",
       "      <td>0.926780</td>\n",
       "      <td>1.462178</td>\n",
       "      <td>1.194479</td>\n",
       "      <td>1.722480</td>\n",
       "      <td>1.722480</td>\n",
       "      <td>1.722480</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1503010</td>\n",
       "      <td>1503214</td>\n",
       "      <td>-</td>\n",
       "      <td>0.657821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>TCTCAAAATCGGTGGAGCAGCATGACAAAGTCATCGGGCATTATCT...</td>\n",
       "      <td>0.893931</td>\n",
       "      <td>0.916526</td>\n",
       "      <td>0.905228</td>\n",
       "      <td>9.341211</td>\n",
       "      <td>9.341211</td>\n",
       "      <td>9.341211</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>278257</td>\n",
       "      <td>278469</td>\n",
       "      <td>-</td>\n",
       "      <td>0.036012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ATGGGATAACGTTACCAGCGGCTTGATCCCCGCCTGCGCCATTTCA...</td>\n",
       "      <td>1.222270</td>\n",
       "      <td>0.644622</td>\n",
       "      <td>0.933446</td>\n",
       "      <td>1.193019</td>\n",
       "      <td>1.193019</td>\n",
       "      <td>1.193019</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3901346</td>\n",
       "      <td>3901619</td>\n",
       "      <td>+</td>\n",
       "      <td>0.923037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>CTTTACTTTCTGAGGCGCGCCAGCCCGCGAGGAAAACAATCTGAAC...</td>\n",
       "      <td>6.035887</td>\n",
       "      <td>6.730085</td>\n",
       "      <td>6.382986</td>\n",
       "      <td>1.098910</td>\n",
       "      <td>1.098910</td>\n",
       "      <td>1.098910</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1644301</td>\n",
       "      <td>1644558</td>\n",
       "      <td>+</td>\n",
       "      <td>0.157059</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            fragment  RNA_exp_1  RNA_exp_2  \\\n",
       "0  CTTCCAACCCATGGCGCGTGCGTACATAAAGGTTTCGGACGCGCGC...   1.316755   1.635498   \n",
       "1  TGACTTCTTCGTGAACTTGCTGGATATGCGTTACGAGTGGAAAGCG...   0.776481   0.881533   \n",
       "2  TGGGGGGATGTATGGGTACGTTGTAATTAGGGATTTAACGAATTAG...   0.530471   1.426090   \n",
       "3  AAAAATAAAATTCAGCAATCAATTAATGCCTTACATCAACATGGCA...   2.392780   2.118698   \n",
       "4  TTTTTTGTCGTTGACCTCACCATGTCGATCACTGTGCCTGTATCCC...   0.664520   0.619367   \n",
       "5  AAAGACAAAGCACTTCTTCAAGAGTTGTGCTTTGGCGTACTGCGTA...   0.686980   0.558180   \n",
       "6  CTATTTGCTCGCAGGTCTGTTTGGTAGTGCCATTACCGGGATGATG...   0.926780   1.462178   \n",
       "7  TCTCAAAATCGGTGGAGCAGCATGACAAAGTCATCGGGCATTATCT...   0.893931   0.916526   \n",
       "8  ATGGGATAACGTTACCAGCGGCTTGATCCCCGCCTGCGCCATTTCA...   1.222270   0.644622   \n",
       "9  CTTTACTTTCTGAGGCGCGCCAGCCCGCGAGGAAAACAATCTGAAC...   6.035887   6.730085   \n",
       "\n",
       "   RNA_exp_ave  DNA_sum_1  DNA_sum_2   DNA_ave  num_mapped_barcodes  \\\n",
       "0     1.476127   1.006226   1.006226  1.006226                    1   \n",
       "1     0.829007   2.227730   2.227730  2.227730                    4   \n",
       "2     0.978280   2.721586   2.721586  2.721586                    3   \n",
       "3     2.255739   5.572886   5.572886  5.572886                    2   \n",
       "4     0.641943   1.163899   1.163899  1.163899                    2   \n",
       "5     0.622580   6.865710   6.865710  6.865710                    2   \n",
       "6     1.194479   1.722480   1.722480  1.722480                    3   \n",
       "7     0.905228   9.341211   9.341211  9.341211                    2   \n",
       "8     0.933446   1.193019   1.193019  1.193019                    1   \n",
       "9     6.382986   1.098910   1.098910  1.098910                    2   \n",
       "\n",
       "   num_integrated_barcodes    start      end strand  variation  \n",
       "0                        1  1838405  1838702      -   0.312743  \n",
       "1                        1  4133776  4134009      +   0.183064  \n",
       "2                        1  2862818  2863076      -   1.426718  \n",
       "3                        1  3798297  3798568      -   0.175510  \n",
       "4                        1  3657099  3657389      +   0.101518  \n",
       "5                        1  3433342  3433583      +   0.299536  \n",
       "6                        1  1503010  1503214      -   0.657821  \n",
       "7                        1   278257   278469      -   0.036012  \n",
       "8                        1  3901346  3901619      +   0.923037  \n",
       "9                        1  1644301  1644558      +   0.157059  "
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761f55c1-a68b-485f-826f-09a11af50b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sbol_doc(df, file_path, media=\"LB\"):\n",
    "    doc = sbol2.Document()\n",
    "    sbol2.setHomespace('http://github.com/cywlol/promoters')\n",
    "    doc.displayId = \"E_coli_promoters\"\n",
    "    # save labels for later when we do the attachments through api\n",
    "    exp_data_labels = []\n",
    "    attachment_file_names = []\n",
    "\n",
    "    media_label_MD = media\n",
    "    chassis_label_MD = \"E_coli_chassis\"\n",
    "\n",
    "    # define the media once, then have it as a reference for each promoter row\n",
    "    media_md = sbol2.ModuleDefinition(media_label_MD)\n",
    "    doc.addModuleDefinition(media_md)\n",
    "    media_md.addRole(\"http://identifiers.org/ncit/NCIT:C48164\") \n",
    "    \n",
    "    # define the chassis once, then have it as a reference for each promoter row\n",
    "    chassis_md = sbol2.ModuleDefinition(chassis_label_MD) \n",
    "    doc.addModuleDefinition(chassis_md)\n",
    "    chassis_md.addRole(\"http://identifiers.org/ncit/NCIT:C14419\")\n",
    "    \n",
    "    #attach genome \n",
    "    chassis_md.wasDerivedFrom = [\"https://www.ncbi.nlm.nih.gov/Taxonomy/Browser/wwwtax.cgi?id=511145\"]\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        fragment_seq = row[\"fragment\"]\n",
    "\n",
    "        # Establish the identities of each component, ensuring that each one is unique\n",
    "        promoter_label_CD = f\"promoter_{i}\"\n",
    "        promoter_seq_label = f\"promoter_seq_{i}\"\n",
    "        sample_design_label_CD = f\"sample_design_{i}\"\n",
    "        strain_label_MD = f\"strain_{i}\"\n",
    "        data_label = f\"exp_data_{i}\"\n",
    "        exp_label = f'sample_{i}_expression_data'\n",
    "        attachment_file_name = f'frag_{media}_exp_sample_{i}.csv'\n",
    "\n",
    "        #engr_region_label_CD = f\"engr_region_{i}\"\n",
    "        #location_promoter_annotation = f\"location_promoter_annotation_{i}\"\n",
    "        #location_promoter_label = f\"location_promoter_label_{i}\"\n",
    "        \n",
    "        # Promoter and Sequence \n",
    "        promoter_cd = sbol2.ComponentDefinition(promoter_label_CD, sbol2.BIOPAX_DNA)\n",
    "        promoter_cd.roles = [sbol2.SO_PROMOTER]\n",
    "        seq = sbol2.Sequence(promoter_seq_label, fragment_seq, sbol2.SBOL_ENCODING_IUPAC)\n",
    "        doc.addSequence(seq)\n",
    "        promoter_cd.sequences = [seq.persistentIdentity]\n",
    "        doc.addComponentDefinition(promoter_cd)\n",
    "\n",
    "        \n",
    "        # Add strain module definition\n",
    "        strain_md = sbol2.ModuleDefinition(strain_label_MD)\n",
    "        strain_md.addRole(\"http://identifiers.org/ncit/NCIT:C14419\")\n",
    "        doc.addModuleDefinition(strain_md)\n",
    "        strain_c1 = strain_md.modules.create('chassis')\n",
    "        strain_c1.definition = chassis_md.persistentIdentity\n",
    "        strain_c2 = strain_md.functionalComponents.create('promoter')\n",
    "        strain_c2.definition = promoter_cd.persistentIdentity\n",
    "    \n",
    "        #annotation = sbol2.SequenceAnnotation(\"promoter_location\")\n",
    "        #range = sbol2.Range(\"prange\", start, end)\n",
    "    \n",
    "        #if (strand == \"-\"):\n",
    "        #    range.orientation = sbol2.SBOL_ORIENTATION_REVERSE_COMPLEMENT\n",
    "                \n",
    "        #annotation.locations.add(range)\n",
    "        #promoter_cd.sequenceAnnotations.add(annotation)\n",
    "        '''\n",
    "        # Engineered Region \n",
    "        engineered_cd = sbol2.ComponentDefinition(engr_region_label_CD, sbol2.BIOPAX_DNA)\n",
    "        engineered_cd.roles = [\"https://identifiers.org/so/SO:0000804\"]\n",
    "        sub = engineered_cd.components.create('promoter')\n",
    "        sub.definition = promoter_cd.persistentIdentity\n",
    "        doc.addComponentDefinition(engineered_cd)\n",
    "        # No sequence?\n",
    "        '''\n",
    "        \n",
    "        #strain_c2 = strain_md.functionalComponents.create('engineered_region')\n",
    "        #strain_c2.definition = strain_md.persistentIdentity\n",
    "        \n",
    "        # Sample Design  \n",
    "        sample_md = sbol2.ModuleDefinition(sample_design_label_CD)\n",
    "        doc.addModuleDefinition(sample_md)\n",
    "        sample_md.addRole(\"http://identifiers.org/obo/OBI:0000073\")\n",
    "\n",
    "        m_strain = sample_md.modules.create('strain')\n",
    "        m_strain.definition = strain_md.persistentIdentity\n",
    "        \n",
    "        m_media = sample_md.modules.create('media')\n",
    "        m_media.definition = media_md.persistentIdentity\n",
    "\n",
    "        # Create the attachment data as a csv\n",
    "        rna_series = row[[\"RNA_exp_1\", \"RNA_exp_2\", \"RNA_exp_ave\"]]\n",
    "        rna_df = pd.DataFrame([rna_series])  \n",
    "        rna_df.to_csv(os.path.join(attachments_path, attachment_file_name), index=False)\n",
    "        \n",
    "        # exp_attachment = sbol2.Attachment(exp_label)\n",
    "        # exp_attachment.name = f'fragmentation expression at rLP5 for sample {i}'\n",
    "        # exp_attachment.description = 'CSV including the gene expression of the sequence: RNA_exp1, RNA_exp2, and its average.'\n",
    "        # exp_attachment.source = 'CSV_LINK_HERE' # update when added attachment to SBOL collection\n",
    "        # exp_attachment.format = 'https://identifiers.org/edam/format_3752'\n",
    "        # doc.addAttachment(exp_attachment)\n",
    "        \n",
    "        # Experiment and Measurement Data\n",
    "        exp = sbol2.ExperimentalData(exp_label)\n",
    "        exp.wasDerivedFrom = sample_md.persistentIdentity\n",
    "        doc.add(exp)\n",
    "\n",
    "        exp_data_labels.append(exp_label)\n",
    "        attachment_file_names.append(attachment_file_name)\n",
    "\n",
    "        if (i == 5):\n",
    "            break\n",
    "            \n",
    "    report = doc.validate()\n",
    "    if (report == 'Valid.'):\n",
    "        doc.write(file_path)\n",
    "    else:\n",
    "        print(report)\n",
    "    return exp_data_labels, attachment_file_names, doc, chassis_label_MD\n",
    "        \n",
    "def partshop_attach_exp_data(synbio_username, collection_name, file_names, email, password, exp_data_labels, version=1):\n",
    "    shop = sbol2.PartShop(\"https://synbiohub.org\")\n",
    "    print(shop.login(email, password))\n",
    "    exp_labels = [\"ExperimentalData_\" + label for label in exp_data_labels]  \n",
    "    for label, file_name in zip(exp_labels, file_names):\n",
    "        path = os.path.join(attachments_path, file_name)\n",
    "        attachment_uri = f\"https://synbiohub.org/user/{synbio_username}/{collection_name}/{label}/{version}\"\n",
    "        shop.attachFile(attachment_uri, path)\n",
    "\n",
    "def partshop_attach_genome_to_md(synbio_username, collection_name, file_path, email, password, chassis_label, version=1):\n",
    "    shop = sbol2.PartShop(\"https://synbiohub.org\")\n",
    "    print(shop.login(email, password))\n",
    "\n",
    "    label = \"ModuleDefinition_\" + chassis_label\n",
    "    attachment_uri = f\"https://synbiohub.org/user/{synbio_username}/{collection_name}/{label}/{version}\"\n",
    "\n",
    "    shop.attachFile(attachment_uri, file_path)\n",
    "\n",
    "def create_synbio_collection(email, password, file_path, id, name, description, version='1'):\n",
    "    response = requests.post(\n",
    "        \"https://synbiohub.org/login\",\n",
    "        headers={\"Accept\": \"text/plain\"},\n",
    "        data={\"email\": email, \"password\": password}\n",
    "    )\n",
    "        \n",
    "    if response.ok:\n",
    "        token = response.text.strip() # theres a whitespace before the token for some reason\n",
    "        response = requests.post(\n",
    "        'https://synbiohub.org/submit',\n",
    "        headers={\n",
    "            'X-authorization': token,\n",
    "            'Accept': 'text/plain'\n",
    "        },\n",
    "        files={\n",
    "        'files': open(file_path,'rb'),\n",
    "        },\n",
    "        data={\n",
    "            'id': id,\n",
    "            'version' : version,\n",
    "            'name' :  name,\n",
    "            'description' : description,\n",
    "            'citations' : '',\n",
    "            'overwrite_merge' : '0'\n",
    "        },\n",
    "    \n",
    "    )\n",
    "    else:\n",
    "        print(\"Login failed:\", response.status_code)\n",
    "        print(response.text)\n",
    "\n",
    "def partshop_pull(email, password, synbio_username, collection_name, file_path, version=1):\n",
    "    shop = sbol2.PartShop(\"https://synbiohub.org\")\n",
    "    doc = sbol2.Document()\n",
    "    shop.login(email, password)\n",
    "\n",
    "    collection_uri = f\"https://synbiohub.org/user/{synbio_username}/{collection_name}/{collection_name}_collection/{version}\"\n",
    "    s = shop.pull(collection_uri, doc)\n",
    "    \n",
    "    for obj in doc:\n",
    "        print(obj)   \n",
    "    \n",
    "    doc.write(file_path)\n",
    "\n",
    "def download_all_attachments(email, password, doc, file_path):\n",
    "    shop = sbol2.PartShop(\"https://synbiohub.org\")\n",
    "    shop.login(email, password)\n",
    "    for attachment in doc.attachments:\n",
    "        shop.downloadAttachment(attachment.identity, filepath=file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bf198d9-d05a-4a6a-9d2e-fad529266188",
   "metadata": {},
   "outputs": [],
   "source": [
    "ecoli_genome_file_name = \"E. coli.fasta\"\n",
    "env_email = os.getenv(\"SYNBIO_EMAIL\")\n",
    "env_password = os.getenv(\"SYNBIO_PASSWORD\")\n",
    "\n",
    "username = \"cywong\"\n",
    "output_name = 'ecolipromoters.xml'\n",
    "id = \"Ecolipromoterexpdata\"\n",
    "name = \"E coli promoter data exploration\"\n",
    "description = \"A collection containing the extracted E coli data from paper\"\n",
    "sbol_file_name = output_name\n",
    "imported_sbol_file_name = \"promoters_import.xml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0de59a6b-c443-4788-a8f0-0d1235fab7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_labels, attachment_file_names, doc, chassis_label = create_sbol_doc(df, os.path.join(sbol_path, output_name))       \n",
    "#create_synbio_collection(env_email, env_password, os.path.join(sbol_path, sbol_file_name), id, name, description)\n",
    "#partshop_attach_exp_data(username, id, attachment_file_names, env_email, env_password, exp_labels)\n",
    "#partshop_attach_genome_to_md(username, id, os.path.join(attachments_path, ecoli_genome_file_name), env_email, env_password, chassis_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36c0cfd1-3334-4808-bb9e-2b20ef3c35b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://synbiohub.org/user/cywong/Ecolipromoterexpdata/Ecolipromoterexpdata_collection/1\n",
      "https://synbiohub.org/user/cywong/Ecolipromoterexpdata/ModuleDefinition_strain_4/1\n",
      "https://synbiohub.org/user/cywong/Ecolipromoterexpdata/ModuleDefinition_strain_2/1\n",
      "https://synbiohub.org/user/cywong/Ecolipromoterexpdata/ModuleDefinition_sample_design_0/1\n",
      "https://synbiohub.org/user/cywong/Ecolipromoterexpdata/ModuleDefinition_sample_design_4/1\n",
      "https://synbiohub.org/user/cywong/Ecolipromoterexpdata/ModuleDefinition_strain_1/1\n",
      "https://synbiohub.org/user/cywong/Ecolipromoterexpdata/ModuleDefinition_E_coli_chassis/1\n",
      "https://synbiohub.org/user/cywong/Ecolipromoterexpdata/ModuleDefinition_sample_design_5/1\n",
      "https://synbiohub.org/user/cywong/Ecolipromoterexpdata/ModuleDefinition_LB/1\n",
      "https://synbiohub.org/user/cywong/Ecolipromoterexpdata/ModuleDefinition_sample_design_1/1\n",
      "https://synbiohub.org/user/cywong/Ecolipromoterexpdata/ModuleDefinition_sample_design_3/1\n",
      "https://synbiohub.org/user/cywong/Ecolipromoterexpdata/ModuleDefinition_strain_5/1\n",
      "https://synbiohub.org/user/cywong/Ecolipromoterexpdata/ModuleDefinition_strain_3/1\n",
      "https://synbiohub.org/user/cywong/Ecolipromoterexpdata/ModuleDefinition_sample_design_2/1\n",
      "https://synbiohub.org/user/cywong/Ecolipromoterexpdata/ModuleDefinition_strain_0/1\n",
      "https://synbiohub.org/user/cywong/Ecolipromoterexpdata/ComponentDefinition_promoter_0/1\n",
      "https://synbiohub.org/user/cywong/Ecolipromoterexpdata/ComponentDefinition_promoter_4/1\n",
      "https://synbiohub.org/user/cywong/Ecolipromoterexpdata/ComponentDefinition_promoter_1/1\n",
      "https://synbiohub.org/user/cywong/Ecolipromoterexpdata/ComponentDefinition_promoter_2/1\n",
      "https://synbiohub.org/user/cywong/Ecolipromoterexpdata/ComponentDefinition_promoter_3/1\n",
      "https://synbiohub.org/user/cywong/Ecolipromoterexpdata/ComponentDefinition_promoter_5/1\n",
      "https://synbiohub.org/user/cywong/Ecolipromoterexpdata/Sequence_promoter_seq_0/1\n",
      "https://synbiohub.org/user/cywong/Ecolipromoterexpdata/Sequence_promoter_seq_5/1\n",
      "https://synbiohub.org/user/cywong/Ecolipromoterexpdata/Sequence_promoter_seq_1/1\n",
      "https://synbiohub.org/user/cywong/Ecolipromoterexpdata/Sequence_promoter_seq_2/1\n",
      "https://synbiohub.org/user/cywong/Ecolipromoterexpdata/Sequence_promoter_seq_4/1\n",
      "https://synbiohub.org/user/cywong/Ecolipromoterexpdata/Sequence_promoter_seq_3/1\n",
      "https://synbiohub.org/user/cywong/Ecolipromoterexpdata/attachment_e90571e455a94152ae1006628e8d09f3/1\n",
      "https://synbiohub.org/user/cywong/Ecolipromoterexpdata/attachment_b91dc2f4a0ba4a0eb0d8e413512489a9/1\n",
      "https://synbiohub.org/user/cywong/Ecolipromoterexpdata/attachment_09f5cc66637b485cb0ce44ff8f6dbd01/1\n",
      "https://synbiohub.org/user/cywong/Ecolipromoterexpdata/attachment_7f23834d85cd4a658ac78289a17bc1f1/1\n",
      "https://synbiohub.org/user/cywong/Ecolipromoterexpdata/attachment_e6ced1aa96ce4e05ab6eaf14e5ef7a6c/1\n",
      "https://synbiohub.org/user/cywong/Ecolipromoterexpdata/attachment_f85366a0b60f431db46620067ba8ad9b/1\n",
      "https://synbiohub.org/user/cywong/Ecolipromoterexpdata/attachment_0632184133744ed5b6630a004381e661/1\n",
      "https://synbiohub.org/user/cywong/Ecolipromoterexpdata/ExperimentalData_sample_3_expression_data/1\n",
      "https://synbiohub.org/user/cywong/Ecolipromoterexpdata/ExperimentalData_sample_5_expression_data/1\n",
      "https://synbiohub.org/user/cywong/Ecolipromoterexpdata/ExperimentalData_sample_0_expression_data/1\n",
      "https://synbiohub.org/user/cywong/Ecolipromoterexpdata/ExperimentalData_sample_1_expression_data/1\n",
      "https://synbiohub.org/user/cywong/Ecolipromoterexpdata/ExperimentalData_sample_2_expression_data/1\n",
      "https://synbiohub.org/user/cywong/Ecolipromoterexpdata/ExperimentalData_sample_4_expression_data/1\n"
     ]
    }
   ],
   "source": [
    "partshop_pull(env_email, env_password, username, id, os.path.join(downloaded_sbol_path, imported_sbol_file_name), version=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a7df4bf5-981f-44ff-8a1a-573dd9996bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download_all_attachments(env_email, env_password, doc, pulled_attachments_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "48b8bd54-df14-4eba-a637-138bb295fcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = sbol2.Document()\n",
    "doc.read(os.path.join(downloaded_sbol_path, imported_sbol_file_name))\n",
    "\n",
    "df_new = pd.DataFrame()\n",
    "\n",
    "for exp in doc.experimentalData:\n",
    "    attachment = doc.get(exp.attachments[0])\n",
    "    sample_design = doc.get(exp.wasDerivedFrom[0])\n",
    "\n",
    "    # may need to be changed\n",
    "    strain = doc.get(sample_design).modules[0] if 'strain' in doc.get(sample_design).modules[0].identity else doc.get(sample_design).modules[1]\n",
    "\n",
    "    promoter =  doc.get(doc.get(strain.definition).functionalComponents[0].definition)\n",
    "    promoter_seq = doc.get(promoter.sequences[0]).elements\n",
    "    \n",
    "    df1 = pd.read_csv(os.path.join(pulled_attachments_path, attachment.name))\n",
    "    df1['Sequence'] = promoter_seq\n",
    "    df_new = pd.concat([df_new, df1], ignore_index=True)\n",
    "\n",
    "\n",
    "# for mod in doc.moduleDefinitions:\n",
    "#     print(\"Mod: \", mod.identity)\n",
    "# for attachment in doc.attachments:\n",
    "#     print(\"Attachment: \", attachment.source) \n",
    "    \n",
    "# for seq in doc.sequences:\n",
    "#     print(\"Seq:\", seq.elements) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fc6333-4cb4-4abf-a9a8-bdb50c2a4051",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.read_csv(os.path.join(data_path, \"endo_scramble_expression_formatted_std.txt\"), delimiter= \"\\t\")\n",
    "df3\n",
    "\n",
    "# Positive promoter controls are synthetic thus have no actual location data. This refers to the 2,000 out of the 17,000 TSS they found, scrambled them to find the functional sites, including controls, scrambled variants, and unscrambled\n",
    "    # name: ID represented as {[TSS name][genomic position][strand + the scrambled region]}\n",
    "    # tss_name: Name of the original (unscrambled) transcription start site (TSS) this variant is derived from.\n",
    "    # tss_position: Start genome coordinate of the TSS.\n",
    "    # strand: Strand direction\n",
    "    # scramble_start: Start of the 10 bp scrambled window relative to var_left and var_right\n",
    "    # scramble_end: End of the scrambled window relative to var_left and var_right\n",
    "    # var_left: Genomic coordinate of the start (5' end) of the full 150 bp promoter variant.\n",
    "    # var_right: Genomic coordinate of the end (3' end) of the full 150 bp promoter variant.\n",
    "    # scramble_start_pos: Genomic coordinate where the scrambled 10 bp region begins.\n",
    "    # scramble_end_pos: Genomic coordinate where the scrambled 10 bp region ends.\n",
    "    # scramble_pos_rel_tss: Position of the scrambled region relative to the TSS.\n",
    "    # variant: The sequence\n",
    "    # RNA_exp_sum_1_1: Sum of RNA expression counts for barcodes in replicate 1, technical replicate 1 (normalized by DNA?)\n",
    "    # RNA_exp_sum_1_2: Sum of RNA expression counts for barcodes in replicate 1, technical replicate 2.\n",
    "    # RNA_exp_sum_2_1: Sum of RNA expression counts for barcodes in replicate 2, technical replicate 1.\n",
    "    # RNA_exp_sum_2_2: Sum of RNA expression counts for barcodes in replicate 2, technical replicate 2.\n",
    "    # RNA_exp_sum_1: Sum of RNA expression across both technical replicates for biological replicate 1.\n",
    "    # RNA_exp_sum_2: Sum of RNA expression across both technical replicates for biological replicate 2.\n",
    "    # RNA_exp_sum_ave: (RNA_exp_sum_1 + RNA_exp_sum_2)/2)\n",
    "    # DNA_1: DNA-seq count in biological replicate 1 (how much of this fragment was integrated).\n",
    "    # DNA_2: DNA-seq count in biological replicate 2\n",
    "    # DNA_ave: (DNA_1 + DNA_2) / 2\n",
    "    # expn_med: Median RNA expression across all barcodes, normalized by DNA abundance (RNA/DNA).\n",
    "    # num_barcodes_integrated: Number of barcodes actually integrated and measured in DNA/RNA-seq.\n",
    "    # category: Label indicating whether this is scrambled, unscrambled, negative, positive control\n",
    "    # unscrambled_exp: Measured promoter activity of the wild-type (unscrambled) version of this promoter.\n",
    "    # relative_exp: (expn_med / unscrambled_exp)\n",
    "\n",
    "df4 = pd.read_csv(os.path.join(data_path, \"fLP3_Endo2_lb_expression_formatted_std.txt\"), delimiter=\"\\t\")\n",
    "df4\n",
    "# Expression summary for the TSS promoter library (17,635 reported TSSs) in LB, integrated at the fLP3 site\n",
    "    # name: Unique identifier for the promoter variant in the format [TSS_name,TSS_position,strand]\n",
    "    # tss_name: The named identifier for the transcription start site (TSS), such as from RegulonDB or Storz \n",
    "    # tss_position: Genomic coordinate of the TSS — the position where transcription starts.\n",
    "    # strand: Strand direction\n",
    "    # start: Absolute genomic coordinate where the 150 bp promoter fragment starts.\n",
    "    # end: Absolute genomic coordinate where the 150 bp promoter fragment ends.\n",
    "    # variant: The promoter sequence\n",
    "    # RNA_exp_sum_1_1: Sum of RNA-seq counts for this variant in biological replicate 1, technical replicate 1.\n",
    "    # RNA_exp_sum_1_2: Sum of RNA-seq counts in biological replicate 1, technical replicate 2.\n",
    "    # RNA_exp_sum_2_1: Sum of RNA-seq counts in biological replicate 2, technical replicate 1.\n",
    "    # RNA_exp_sum_2_2: Sum of RNA-seq counts in biological replicate 2, technical replicate 2.\n",
    "    # RNA_exp_sum_1: Total RNA expression for biological replicate 1 \n",
    "    # RNA_exp_sum_2: Total RNA expression for biological replicate 2 \n",
    "    # RNA_exp_sum_ave: Average of RNA expression across the two biological replicates.\n",
    "    # DNA_1: DNA-seq count in biological replicate 1 \n",
    "    # DNA_2: DNA-seq count in biological replicate 2.\n",
    "    # DNA_ave: Average of DNA_1 and DNA_2\n",
    "    # expn_med: Median RNA/DNA expression across barcodes \n",
    "    # num_barcodes_integrated: Number of barcodes that were actually integrated and sequenced.\n",
    "    # category: Type of promoter: ['tss', 'neg_control', 'pos_control']\n",
    "    # active: Binary classification — \"active\" or \"inactive\" promoter based on expn_med\n",
    "\n",
    "\n",
    "df5 = pd.read_csv(os.path.join(data_path,\"peak_tile_expression_formatted_std.txt\"), delimiter=\"\\t\")\n",
    "df5\n",
    "# The authors first used sheared genomic fragments (~200–300 bp) to find candidate promoter regions, and then designed a tiling oligo library: ~150 bp sequences overlapping by 10 bp Spanning all ~3,500 candidate promoter region, located at LB and nth-ydgR intergenic locus \n",
    "    # variant: The 150 bp DNA sequence (oligo) used in the MPRA tile\n",
    "    # name: Identifier for the tile, usually formatted as “peak_start_peak_end_strand_posStart-posEnd”.\n",
    "    # peak_start: Genomic start coordinate of the candidate promoter region (the whole peak region).\n",
    "    # peak_end: Genomic end coordinate of the candidate promoter region.\n",
    "    # strand: Strand direction\n",
    "    # tile_start: Start of this 150 bp oligo relative to the peak\n",
    "    # tile_end: End of the tile \n",
    "    # RNA_exp_sum_1_1: Sum of RNA reads for this tile in biological replicate 1, technical replicate 1\n",
    "    # RNA_exp_sum_1_2: Same as above, but technical replicate 2\n",
    "    # RNA_exp_sum_2_1: Sum of RNA reads in biological replicate 2, technical replicate 1.\n",
    "    # RNA_exp_sum_2_2: Same as above, but technical replicate 2.\n",
    "    # RNA_exp_sum_1: Total RNA expression in biological replicate 1\n",
    "    # RNA_exp_sum_2: Total RNA expression in biological replicate 2 \n",
    "    # RNA_exp_sum_ave: Average RNA expression across the two biological replicates.\n",
    "    # DNA_1: DNA read count in biological replicate 1\n",
    "    # DNA_2: DNA read count in biological replicate 2.\n",
    "    # DNA_ave: Average of DNA_1 and DNA_2\n",
    "    # expn_med: Median RNA/DNA expression across all barcodes for this tile \n",
    "    # num_barcodes_integrated: Number of barcodes that were successfully integrated and sequenced.\n",
    "    # category: ['tile', 'neg_control', 'random', 'pos_control']\n",
    "    # peak_length: Length (in bp) of the peak region from which this tile was derived (e.g., 362 bp).\n",
    "    # tile_start_relative: Position of the tile’s start within the peak (tile_start / peak_length)\n",
    "    # start: Genomic start coordinate of the tile (based on tile_start + peak_start).\n",
    "    # end: Genomic end coordinate of the tile.\n",
    "    # active: Binary label indicating if the tile is active or inactive\n",
    "\n",
    "df6 = pd.read_csv(os.path.join(data_path,\"rLP5_Endo2_lb_expression_formatted_std.txt\"), delimiter=\"\\t\")\n",
    "df6\n",
    "# Expression summary for the TSS promoter library (17,635 reported TSSs) in LB, integrated at the rLP5 site\n",
    "\n",
    "df7 = pd.read_csv(os.path.join(data_path, \"rLP6_Endo2_lb_expression_formatted_std.txt\"), delimiter=\"\\t\")\n",
    "df7\n",
    "# Expression summary for the TSS promoter library (17,635 reported TSSs) in LB, integrated at the rLP6 site\n",
    "\n",
    "        \n",
    "def post_all_attachments(synbio_username, collection_name, file_names, email, password, exp_data_labels, version=1):\n",
    "    sbh_url = \"https://synbiohub.org/login\" \n",
    "    sbh_email = email\n",
    "    sbh_password = password\n",
    "    \n",
    "    response = requests.post(\n",
    "        sbh_url,\n",
    "        headers={\"Accept\": \"text/plain\"},\n",
    "        data={\"email\": sbh_email, \"password\": sbh_password}\n",
    "    )\n",
    "\n",
    "    if response.ok:\n",
    "        token = response.text.strip() # theres a whitespace before the token for some reason\n",
    "        print(token)\n",
    "        exp_labels = [\"ExperimentalData_\" + label for label in exp_data_labels]  \n",
    "        \n",
    "        for label, file_name in zip(exp_labels, file_names):\n",
    "            attachment_uri = f\"https://synbiohub.org/user/{synbio_username}/{collection_name}/{label}/{version}/attach\"\n",
    "            print(attachment_uri)\n",
    "            with open(file_name, 'rb') as f:\n",
    "                response = requests.post(\n",
    "                    attachment_uri,\n",
    "                    headers={\n",
    "                        'X-authorization': token,\n",
    "                        'Accept': 'text/plain'\n",
    "                    },\n",
    "                    files={'file': f}\n",
    "                )\n",
    "    \n",
    "            \n",
    "            print(\"Status Code:\", response.status_code)\n",
    "            print(\"Response Body:\", response.text)\n",
    "    else:\n",
    "        print(\"Login failed:\", response.status_code)\n",
    "        print(response.text)\n",
    "\n",
    "def post_attachment(attachment_uri, file_path, email, password):\n",
    "    sbh_url = \"https://synbiohub.org/login\" \n",
    "    sbh_email = email\n",
    "    sbh_password = password\n",
    "    \n",
    "    response = requests.post(\n",
    "        sbh_url,\n",
    "        headers={\"Accept\": \"text/plain\"},\n",
    "        data={\"email\": sbh_email, \"password\": sbh_password}\n",
    "    )\n",
    "    \n",
    "    if response.ok:\n",
    "        token = response.text.strip() # theres a whitespace before the token for some reason\n",
    "        print(token)\n",
    "\n",
    "        with open(file_path, 'rb') as f:\n",
    "            response = requests.post(\n",
    "                attachment_uri,\n",
    "                headers={\n",
    "                    'X-authorization': token,\n",
    "                    'Accept': 'text/plain'\n",
    "                },\n",
    "                files={'file': f}\n",
    "            )\n",
    "        \n",
    "        print(\"Status Code:\", response.status_code)\n",
    "        print(\"Response Body:\", response.text)\n",
    "    \n",
    "    else:\n",
    "        print(\"Login failed:\", response.status_code)\n",
    "        print(response.text)\n",
    "\n",
    "#df1 and df2\n",
    "\n",
    "# fragment: 150–300 bp genomic DNA sequence that were randomly sheared and barcoded\n",
    "# RNA_exp_1: RNA expression level (replicate 1) – normalized measurement of transcript abundance for this fragment in the first RNA-Seq replicate. (normalized by DNA)\n",
    "# RNA_exp_2: RNA expression level (replicate 2) – same as above, but from a second biological replicate.\n",
    "# RNA_exp_ave: Average RNA expression – mean of RNA_exp_1 and RNA_exp_2\n",
    "# DNA_sum_1: DNA integration abundance (replicate 1) – quantifies how much of this DNA fragment (or barcode) was actually integrated in the first DNA-Seq sample. \n",
    "# DNA_sum_2: DNA integration abundance (replicate 2) – same as above, from the second replicate.\n",
    "# DNA_ave: Average DNA integration level – mean of DNA_sum_1 and DNA_sum_2\n",
    "# num_integrated_barcodes: Number of barcodes that were integrated into the genome for this fragment. Typically should match num_mapped_barcodes unless there are sequencing/integration issues.\n",
    "# start: Start coordinate (in bp) in the E. coli MG1655 reference genome (U00096.2). \n",
    "# end: End coordinate in the E. coli MG1655 reference genome (U00096.2). \n",
    "# strand: DNA strand orientation (+ or -) \n",
    "# variation: Standard deviation or variability in RNA measurement across barcode replicates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e292392c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "import numpy as np\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "import tempfile\n",
    "# matplotlib.use('pdf')\n",
    "# import matplotlib.pyplot as plt\n",
    "from abc import abstractmethod, ABCMeta\n",
    "from sklearn.tree import DecisionTreeClassifier as scikit_DecisionTree\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "class Model(object):\n",
    "    __metaclass__ = ABCMeta\n",
    "\n",
    "    @abstractmethod\n",
    "    def __init__(self, **hyperparameters):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def train(self, X, y, validation_data):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(self, X):\n",
    "        pass\n",
    "\n",
    "    def test(self, X, y):\n",
    "        return self.evaluate(X, y)\n",
    "        # return ClassificationResult(y, self.predict(X))\n",
    "\n",
    "    def score(self, X, y):\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "class DecisionTree(Model):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.classifier = scikit_DecisionTree()\n",
    "\n",
    "    def train(self, X, y, validation_data=None):\n",
    "        self.classifier.fit(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = np.asarray(self.classifier.predict_proba(X))[..., 1]\n",
    "        if len(predictions.shape) == 2:  # multitask\n",
    "            predictions = predictions.T\n",
    "        else:  # single-task\n",
    "            predictions = np.expand_dims(predictions, 1)\n",
    "        return predictions\n",
    "    \n",
    "\n",
    "class RandomForestRegression(DecisionTree):\n",
    "    def __init__(self):\n",
    "        self.regressor = RandomForestRegressor(n_estimators=100)\n",
    "\n",
    "    def train(self, X, y, validation_data=None):\n",
    "        # X shape: n_samples, n_features\n",
    "        # y shape: n_samples\n",
    "        self.regressor.fit(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.regressor.predict(X)\n",
    "\n",
    "    def score(self, X, y):\n",
    "        predictions = np.squeeze(self.regressor.predict(X))\n",
    "        return np.corrcoef(predictions, y)[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef3a6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode_modern(sequences):\n",
    "    sequences_array = np.array([[char for char in seq] for seq in sequences])\n",
    "    sequence_length = sequences_array.shape[1]\n",
    "    num_samples = sequences_array.shape[0]\n",
    "\n",
    "    defined_categories = ['A', 'C', 'G', 'T', 'N']\n",
    "    ohe = OneHotEncoder(sparse_output=False,\n",
    "                        categories=[defined_categories] * sequence_length,\n",
    "                        dtype=np.float32) # Still keep this as float32\n",
    "\n",
    "    return ohe.fit_transform(sequences_array)\n",
    "\n",
    "def pad_sequence(seq, max_length):\n",
    "\tif len(seq) > max_length:\n",
    "\t\tdiff = len(seq) - max_length\n",
    "\t\ttrim_length = int(diff / 2)\n",
    "\t\tseq = seq[trim_length : -(trim_length + diff%2)]\n",
    "\telse:\n",
    "\t\tseq = seq.center(max_length, 'N')\n",
    "\treturn seq\n",
    "\n",
    "def process_seqs(df, seq_length):\n",
    "\tpadded_seqs = [pad_sequence(x, seq_length) for x in df['variant']]\n",
    "\tX = one_hot_encode_modern(np.array(padded_seqs))\n",
    "\ty = np.array(df['expn_med_fitted_scaled'], dtype=np.float32)\n",
    "\treturn X, y\n",
    "\n",
    "\n",
    "def split_data_by_peak(data_df):\n",
    "    np.random.seed(123)\n",
    "\n",
    "    # 1. Create 'peak_name' column (if not already present)\n",
    "    # Assuming peak_start and peak_end are numeric, convert to string for concatenation\n",
    "    data_df['peak_name'] = data_df['peak_start'].astype(str) + 'to' + data_df['peak_end'].astype(str)\n",
    "\n",
    "    # 2. Get unique peak names\n",
    "    peak_names = data_df['peak_name'].unique()\n",
    "\n",
    "    # 3. Randomly sample train peak names (90%)\n",
    "    train_size = int(0.90 * len(peak_names))\n",
    "    train_peak_names = np.random.choice(peak_names, size=train_size, replace=False)\n",
    "\n",
    "    # 4. Determine test peak names\n",
    "    test_peak_names = np.array([p for p in peak_names if p not in train_peak_names])\n",
    "\n",
    "\n",
    "    data_train = data_df[data_df['peak_name'].isin(train_peak_names)].copy()\n",
    "    data_test = data_df[data_df['peak_name'].isin(test_peak_names)].copy()\n",
    "\n",
    "    return data_train, data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0490a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_df = df5\n",
    "train_df, test_df = split_data_by_peak(data_df.copy())\n",
    "\n",
    "X_train, y_train = process_seqs(train_df, 150)\n",
    "X_test, y_test = process_seqs(test_df, 150)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9826fbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np, random\n",
    "np.random.seed(1)\n",
    "random.seed(1)\n",
    "from sklearn.model_selection import train_test_split  # sklearn >= 0.18\n",
    "import sys\n",
    "import argparse\n",
    "import time\n",
    "\n",
    "def train_RandomForest(X_train, X_test, y_train, y_test):\n",
    "    print(\"Running random forest regression...\")\n",
    "    model = RandomForestRegression()\n",
    "    model.train(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    \n",
    "    with open(\"outputs.txt\", 'w') as outfile:\n",
    "        for i in range(len(predictions)):\n",
    "            outfile.write(str(float(predictions[i])) + '\\t' +\n",
    "                      str(float(y_test[i])) + '\\n')\n",
    "    \n",
    "    score = model.score(X_test, y_test)\n",
    "    print(\"Score:\", score)\n",
    "    return model, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb56143f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, predictions = train_RandomForest(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab152ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "r_squared = r2_score(y_test, predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"Root Mean Square Error (RMSE): {rmse}\")\n",
    "print(f\"R^2: {r_squared}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
